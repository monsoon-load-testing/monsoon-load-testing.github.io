<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">

<head>
  <meta charset="utf-8" />
  <title>Monsoon Load Testing • Case Study</title>
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:regular,500,600,700" media="all" />
  <script type="text/javascript">
    WebFont.load({ google: { families: ["Inter:regular,500,600,700"] } });
  </script>
  <script type="text/javascript">
    !(function (o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      (n.className += t + "js"),
        ("ontouchstart" in o ||
          (o.DocumentTouch && c instanceof DocumentTouch)) &&
        (n.className += t + "touch");
    })(window, document);
  </script>
  <link href="assets/images/monsoon_graphic_color-cropped.svg" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/images/monsoon_graphic_color-cropped.svg" rel="apple-touch-icon" />
  <script src="https://kit.fontawesome.com/d019875f94.js" crossorigin="anonymous"></script>
  <meta name="image" property="og:image" content="assets/images/monsoon_graphic_color-cropped.svg" />
</head>

<body>
  <div class="navigation-wrap">
    <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
      <div class="navigation-container">
        <div class="navigation-left">
          <a href="/" aria-current="page" class="brand w-nav-brand w--current" aria-label="home">
            <img src="assets/images/monsoon_graphic_color.svg" alt="" class="template-logo">
          </a>
          <nav role="navigation" class="nav-menu w-nav-menu">
            <a href="./case-study.html" class="link-block w-inline-block">
              <div>Case Study</div>
            </a>
            <a href="./team.html" class="link-block w-inline-block">
              <div>The Team</div>
            </a>
          </nav>
        </div>
        <div class="navigation-right">
          <div class="login-buttons">
            <a href="https://github.com/monsoon-load-testing/monsoon" target="_blank">
              <span style="color: #161d6f">
                <i class="fab fa-github fa-lg"></i>
              </span>
            </a>
          </div>
        </div>
      </div>
      <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
    </div>
  </div>
  <div id="sidebar" class="toc">
  </div>
  <div class="section header">
    <article class="container case-study-container">
      <div class="hero-text-container">
        <h1 class="h1 centered">Case Study</h1>
      </div>
      <div id="case-study">
        <br />
        <br />
        <!-- Section 1 -->
        <h2 class="h2">1 Introduction</h2>
        <br>
        <p>
          Load tests are a critical tool for any company that wants to build and maintain robust, 
          scalable websites and web apps. They simulate traffic hitting your servers, 
          allowing you to measure exactly how your app performs under different load profiles.
        </p>
        <br>
        <p>
          The overall concept of load testing was born in the physical world. 
          For example, structural engineers load test bridges before opening them to the public and power engineers load test electrical distribution systems. 
        </p>
        <br>
        <p>
          But in the world of software, load testing is the process of directing simulated traffic at any application that handles requests, 
          be that a website, web app, API endpoint, even load balancers and caches, then measuring that software’s performance under the load.
        </p>
        <br />
        <h3>1.1 Who’s interested in load test results?</h3>
        <br />
        <p>
          Load test results are valuable to both business stakeholders and engineering teams. 
        </p>
        </br>
        <p>
          On the business side, when an app can’t cope with its current amount of traffic, the app’s performance degrades or the app may stop working entirely. 
          This results in an immediate drop in the company’s revenue.
        </p>
        <br />
        <p>
          From the engineering team’s perspective, load testing provides valuable feedback about your production system - how many concurrent users can it handle? 
          How does response time change as load increases? Is there an uptick in failed requests under heavier load?
        </p>
        <br />
        <p>
          It’s this engineering perspective that we’ll focus on in this case study of Monsoon.
        </p>
        <br />
        <h3>1.2 What is Monsoon?</h3>
        <br />
        <p>
          Monsoon is an open-source, serverless framework for running browser-based load tests in the cloud. 
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/demo.gif" id="case-study" alt="">
          <figcaption>Fig 1.1: Monsoon's dashboard.</figcaption>
        </figure>
        <br />
        <p>
          Monsoon allows software engineers to easily load test their single-page application in anticipation of traffic spikes or overall business growth. 
          Monsoon can simulate loads of up to 20,000 concurrent users, and tests can be of any duration, from minutes to weeks or longer. 
          Engineers can also see their load test results visualized in a near real time dashboard.
        </p>
        <br />
        <h3>1.3 Brain Boost, a Hypothetical User Story</h3>
        <br />
        <p>
          To see load testing in action, let’s spend some time with the engineering team at Boost Health. 
          Boost is a rapidly-growing startup in the health and wellness space.
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/boost_team.svg" class="case-study-image" alt="">
          <figcaption>Fig 1.1: The Team.</figcaption>
        </figure>
        <p>
          The stakes are high. Boost’s marketing team has spent months planning the product launch for its new Brain Boost supplement. 
          It’s Boost’s biggest product launch ever, and business executives want to close new rounds of venture capital funding based on the success of Brain Boost. 
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/brain.svg" class="case-study-image-x-small" alt="">
          <figcaption>Fig 1.2: Going all in on Brain Boost.</figcaption>
        </figure>
        <br />
        <p>
          Boost expects more traffic than their site has ever seen on launch day. 
          They’re predicting peaks of 4,000 concurrent users. 
          The engineers at Boost are tasked with making sure the site can withstand all that traffic, so they decide to run load tests.
        </p>
        <br />
        <p>
          But before we discuss these load tests, it’s important to note that the Boost website is a single page application. 
          This has important implications for load testing.
        </p>
        <br />
        <h3>1.4 What is a Single Page Application?</h3>
        <br />
        <p>A single page application is a web app comprised of just a single HTML page. Unlike a traditional website, after the initial page load, there are no page reloads. 
          The content of the single page is just repeatedly updated on the fly using browser-side JavaScript.
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/SPA_benefits.svg" class="case-study-image-large" alt="">
          <figcaption>Fig 1.3: SPA Benefits.</figcaption>
        </figure>
        <p>
          SPAs have some significant advantages over more traditional websites:
        </p>
        <ul>
          <li>They reduce strain on server resources.</li>
          <li>They speed up development by decoupling the frontend code from the backend code.</li>
          <li>They provide a much more dynamic, responsive experience for the end user since there are no full page reloads.          </li>
        </ul>
        <br />
        <h3>1.5 Boost Health and Their Load Testing Journey</h3>
        <br />
        <p>
          So let’s follow along with the Boost engineers as they load test their SPA.
        </p>
        <br />
        <h4>First Attempt - Protocol-based load testing with JMeter</h4>

        <br />
        <p>
          One of the Boost engineers used a well-established, open-source load testing tool called Apache JMeter at a previous job, 
          so this is the first option the team tries.
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/protocol-based-load-testing.gif" class="case-study-image-large">
          <figcaption>Fig 1.4: Protocol-based load testing</figcaption>
        </figure>
        <p>
          JMeter is categorized as a protocol-based load testing tool. Protocol-based load tests are the original type of load test. 
          They involve traffic simulation at the HTTP protocol layer. 
          For example, if loading a webpage triggers HTTP requests for 75 subresources, with protocol-based testing, the developers will need to write code to request the original page AND all 75 of those subresources.
        </p>
        <br />
        <p>
          The Boost Health engineers want to test a customer adding Brain Boost to her cart, a process that breaks down into 3 different actions:
        </p>
        <ol>
          <li>Go to the Boost Health Main Page</li>
          <li>View the Brain Boost product details</li>
          <li>Add Brain Boost to the cart</li>
        </ol>
        <br/>
        <figure>
          <img src="assets/images/1-introduction/3actions_125requests.gif" class="case-study-image-large">
          <figcaption>Fig 1.5: Bye bye weekend.</figcaption>
        </figure>
        <p>
          To simulate this workflow, the Boost engineers need to program JMeter to send 125 different HTTP requests in the correct order. This is clearly a great deal of work for the team. On top of this, JMeter is a complex tool and the learning curve is steep. The team settles in for many painful days of work ahead. 
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/unhappydev-jmeter.svg" class="case-study-image">
          <figcaption>Fig 1.6: Protocol-based load testing can be painful.</figcaption>
        </figure>
        <br />
        <p>
          After the team gets their first JMeter load test working, their results seem strangely incomplete. It turns out JMeter is a poor choice for load testing SPAs. 
          SPAs are JavaScript-intensive, but JMeter has no JavaScript interpreter and therefore can’t execute any JavaScript code. (The same holds true for other protocol-based load testing tools.) 
          Therefore, the bulk of an SPA is untestable with JMeter. This is a dealbreaker. 
        </p>
        <br />
        <h4>Second Attempt - Browser-based load testing with Selenium</h4>
        <br />
        <p>
          Another engineer on the Boost team knows of a different tool called Selenium and knows that Selenium can be used for browser-based load testing. 
        </p>
        <br />
        <p>
          <Strong>Browser-based load testing</strong> simulates web traffic using real web browsers rather than naked network requests. 
          Since we’re using browser instances to direct traffic to the site being tested, 
          those browsers clearly have built-in JavaScript interpreters and are fully capable of handling SPAs, unlike protocol-based load testing tools.
        </p>
        <br />
        <p>
          An even more fundamental difference between protocol-based load testing and browser-based load testing tools like Selenium is this: 
          Is the core unit we care about, is that the individual network request? Or is it an action the end user takes (which could actually result in 100 or more network requests)?
        </p>
        <br />
        <p>
          In Boost’s case, it’s an action a website user takes, like loading the homepage, 
          viewing product details or clicking the “Add to Cart” button.
        </p>
        <br />
        <p>
          Furthermore, thinking in terms of end user actions rather than lower-level network requests means browser-based load testing is significantly less complex than its protocol-based counterpart. 
          When planning how to test the Boost site, the engineers can think at a higher level of abstraction, which reduces bugs, makes for a better developer experience and saves significant amounts of developer time.
        </p>
        <br />
        <p>
          Returning to Selenium, it's a suite of browser-based test automation tools. It was never actually designed for load testing, but that's how Selenium came to be used by many developers. With Selenium, the Boost engineers don't need to worry about HTTP requests anymore. All they need to do is script their 3 end user actions.​
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/3actions_3actions.gif" class="case-study-image-large">
          <figcaption>Fig 1.7: Ok, that's better.</figcaption>
        </figure>
        <p> 
          The team runs their first browser-based load test using Selenium. 
          It’s a fairly small-scale test simulating 5 users concurrently visiting the Boost Health website. This test goes off without a hitch. 
        </p>
        <br />
        <p>
          Next, they test 100 concurrent users. This test doesn’t go so well. Selenium is a tool that runs locally on one of the engineers’ laptops, and testing 100 users means spinning up 100 browser instances. This is too resource-intensive for a single laptop. 
          And the actual number of users the Boost team needs to test is 4000, not 100. So the team has run out of local computing resources before they’re able to apply sufficient load to the Boost site.
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/unhappydev-selenium.svg" class="case-study-image">
          <figcaption>Fig 1.8: Local browser-based load testing doesn’t cut it.</figcaption>
        </figure>
        <br />
        <p>
          Clearly this is unworkable. Because browser-based load testing is so resource intensive, the Boost engineers need a solution that’s hosted in the cloud.
        </p>
        <br />
        <h4>Third Attempt - Browser-based load testing with Flood</h4>
        <br />
        <p>
          Researching cloud-hosted browser-based load testing, the team quickly discovers a platform called Flood. 
          Flood is an industry leader in the cloud-hosted, browser-based load testing space. Their platform is definitely capable of generating the 4000 concurrent users Boost needs to load test their site. However, Flood is very expensive. 
          We'll return to Flood later, but for now, the cost is a major drawback, enough to rule Flood out.
        </p>
        <figure>
          <img src="assets/images/1-introduction/unhappydev-flood.svg" class="case-study-image">
          <figcaption>Fig 1.9: Flood gets the job done but costs too much.</figcaption>
        </figure>
        <br />
        <h4>The Journey So Far</h4>
        <p>
          Let's summarize where the Boost engineers are right now. They initially tried protocol-based load testing. This was unsuccessful because protocol-based tools can't test an SPA's JavaScript code. 
          Next, they tried local browser-based load testing. 
          This too was unsuccessful because browser-based load testing is too resource intensive for a single machine. Third, they tried browser-based load testing in the cloud with the Flood platform. 
          However, this proved prohibitively expensive. 
        </p>
        <br />
        <p>
          So the team searches for a more economical option for browser-based load testing in the cloud. 
          In short order, they come across an open source tool called Monsoon.
        </p>
        <br />
        <!-- Section 2 -->
        <h2>2 Overview of Monsoon</h2>
        <br />
        <p>
          Monsoon is browser-based load testing platform hosted in the cloud. We built Monsoon to allow small- to medium-sized companies to load test their SPAs in a simple and scalable way, allowing them to get insights into its performance in near real time. ​
        </p>
        <br/>
        <h3>2.1 Four Key Components</h3>
        <p>
          Monsoon uses Amazon Web Services as its cloud provider, and all the Monsoon infrastructure is deployed directly into your AWS account. This infrastructure has 4 key components: Load Generation, Transformation, Storage and Visualization. 
        </p>
        <figure>
          <img src="assets/images/2-overview/monsoon_architecture_all.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.1: Monsoon's 4 key components</figcaption>
        </figure>
        <br />
        <h4>Load Generation</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_load_generation.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.2: Load Generation architecture</figcaption>
        </figure>
        <p>
          A​t the start of the load test, Monsoon provisions multiple compute instances to generate load. The number of instances scales linearly with the needs of the test. Currently Monsoon is able to handle up to 20,000 virtual users.
        </p>
        <br />
        <figure>
          <img src="assets/images/2-overview/cloud_purple.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.3: Headless Chrome instances in the cloud</figcaption>
        </figure>
        <p>
          Inside one of these compute instances, we use a tool called Puppeteer to simulate virtual users visiting the Boost Health website. Puppeteer is a powerful, browser-based testing library for Node, distributed as an npm package. Puppeteer provides us with a high-level API that gives us full control over headless Chrome instances.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/puppeteer_virtual_user.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.4: Monsoon uses Puppeteer to simulate users visiting a website</figcaption>
        </figure>
        <p>
          Puppeteer is very easy to use. For example, to simulate a user filling out a form, you just pass a CSS selector and the data as arguments to the `type` method. And to simulate a user clicking a button, just pass that button’s CSS selector to the `click` method.
        </p>
        <br/>
        <p>
          Thanks to Puppeteer, Monsoon is able to abstract away a lot of the complexity of the load testing process. With Monsoon, you no longer write code to hit specific backend API endpoints. Instead, you write a simple Puppeteer script that describes the end user actions you want to simulate. Then you configure the load test by specifying the number of simulated users and the duration of the test.
        </p>
        <br/>
        <p>
          Once we have the test script and its configuration, Monsoon uses Puppeteer to create and control headless Chrome instances. Each of those Chrome instances executes the actions defined by the Boost engineers in their test script, hitting dozens or hundreds of backend API endpoints in the process.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/puppeteer_load_testing.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.5: Simulated users execute the test script, and Monsoon records the metrics</figcaption>
        </figure>
        <p>
          Then Monsoon records performance metrics for each end user action and saves the data in JSON format. This process repeats for the duration of the load test.
        </p>
        <br/>

        <h4>Transformation</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_transformation.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.6: Transformation architecture</figcaption>
        </figure>
        <p>
          This load generation step can result in a serious amount of data. A one-hour test can easily generate hundreds of millions of data records. Let’s examine how Monsoon’s transformation architecture manages all this data.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/aggregation_pipeline.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.7: Monsoon aggregates and stores test results</figcaption>
        </figure>
        <p>
          Once we have our first raw test results ready, Monsoon performs a pre-processing step inside the compute instances. This readies the results to enter Monsoon's data pipeline, beginning the transformation step we see above. In this step, the test results for all virtual users are aggregated in increments of 15 seconds.​ These aggregated test results then need to be stored in a database.
        </p>
        <br/>

        <h4>Storage</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_storage.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.8: Storage Architecture</figcaption>
        </figure>
        <p>
          or our Storage component, we used a time-series database. This type of database is optimized for time-series data, which is simply data points that are ordered by timestamp. We'll talk more about time-series data and why we opted for a time-series database later in this case study.​
        </p>
        <br/>
        <h4>Visualization</h3>
        <figure>
          <img src="assets/images/2-overview/architecture_visualization.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.9: Monsoon's Weather Channel, a near real time dashboard</figcaption>
        </figure>
        <br/>
        <p>
          Test results stored in the database can be visualized using Monsoon’s Weather Channel. Weather Channel is a locally-hosted dashboard that lets you see how your site actually performed under load.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/metrics.svg" class="case-study-image-full" alt="">
          <figcaption>Fig. 2.10: Weather Channel gives you insight into the performance of your app</figcaption>
        </figure>
        <br/>
        <p>
          Specifically, Weather Channel allows you to track:
        </p>
        <ul>
          <li>Response time</li>
          <li>Concurrent users </li>
          <li>Transaction rate (end user actions performed per minute)</li>
          <li>Pass ratio</li>
        </ul>
        <!-- Section 3-->
        <h2>3 Who should use Monsoon?</h2>
        <br />
        <p>
          Let's take a look at Boost's other browser-based load testing options.
        </p>
        <figure>
          <img src="assets/images/3-who_should_use_monsoon/3_1_SaaS_vs_DIY.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 3.1: Browser-based load testing options</figcaption>
        </figure>
        </br>
        <p>
          There are two typical paths Boost might choose. The first is a cloud-based software as a service solution, like the Flood platform mentioned earlier. The second is a do-it-yourself approach, tasking the Boost engineers with building their own browser-based load testing tool.​
        </p>
        </br>
        <p>
          Let's examine the tradeoffs for a cloud-based SaaS solution first. A platform like Flood is highly scalable, able to simulate tens or even hundreds of thousands of concurrent users. It's also generally very easy to use. It's usually a hosted platform, meaning the end user doesn't need to configure or deploy any infrastructure. This makes the overall process quite straightforward. However the downside is cost. A single test can cost thousands of dollars. And a small-to-medium sized company like Boost may have a difficult time justifying spending that much on yet another SaaS product.​
        </p>
        <br />
        <p>
          Turning to a DIY approach, a major benefit here is that, once the platform is built, operation is much cheaper. You'll only pay for the underlying cloud infrastructure, no ongoing platform fees. However, building a distributed load testing platform requires a lot of engineering hours. Those engineering hours are expensive and would be hard to justify for a company of Boost's size. Furthermore, Boost doesn't have a huge engineering team. To build a distributed load testing tool from scratch, they'll have to pull engineers off projects that are central to their core business.​
        </p>
        <br/>
        <p>
          Neither of these options seems like a good fit for Boost. But what if there were another option? That’s where we think Monsoon fits in.
        </p>
        <figure>
          <img src="assets/images/3-who_should_use_monsoon/3_2_Solutions_comparison_table.svg">
          <figcaption>Fig. 3.2: Monsoon's niche in the browser-based load testing space</figcaption>
        </figure>
        <br/>
        <p>
          Monsoon glues together all the resources needed to create a near real time load testing platform, then deploys it directly to Boost's AWS account. 
        </p>
        <br/>
        <p>
          Thanks to its serverless nature, Monsoon is just as easy to use as existing cloud-based SaaS solutions like Flood. Additionally, since Monsoon would be deployed directly to the Boost AWS account, Boost retains ownership of its data.​
        </p>
        <br/>
        <p>
          However, Monsoon is neither as scalable nor as feature rich as other cloud-based SaaS products or highly-customized DIY solutions. Monsoon is an open source solution intentionally optimized for small- to medium-sized companies that want to load test their SPA for up to 20,000 concurrent users.​
        </p>
        <br/>
        <p>
          The good news is that the Monsoon team has already invested the engineering hours so you don't have to. We also charge no platform fees. You only pay the AWS charges stemming from the infrastructure spun up to run your test. This makes Monsoon significantly more affordable than either cloud-based SaaS solutions or DIY approaches.​
        </p>
        <br/>
        <!-- Section 4-->
        <h2>4 Design Decisions</h2>
        <br />
        <p>
          Early on, we made two design decisions that strongly shaped the Monsoon framework. The first was choosing to focus on high scalability. The second was striving to provide near real time results to engineers using Monsoon.
        </p>
        </br>
        <h3>4.1 Scalability</h3>
        <p>We set out to build a framework able to simulate as many users as possible, for as long as possible, as cheaply as possible. To accomplish this, we focused on the scalability of the load generation engine and of the data pipeline.​</p>
        <br/>
        <h4>Optimizing the Load Generation Engine</h4>
        <figure>
          <img src="assets/images/4-design_decisions/engine_figure.svg">
          <figcaption>Fig. 4.1: How Puppeteer stacks up against Selenium</figcaption>
        </figure>
        <br/>
        <p>​We had several options for simulating virtual users via headless browser instances. Selenium is the industry default. One Selenium app instance is able to simulate 5 virtual users. We compared this to Google's Puppeteer discussed earlier. One Puppeteer app instance is able to simulate 20 virtual users.</p>
        <p>This difference in simulated users per app instance is hugely important. Since each app instance requires its own execution environment, increasing the number of simulated users per app instance reduces the compute resources you need. And since AWS's billing model is "only pay for what you use", using compute resources more efficiently directly leads to cost savings.</p>
        <p>By choosing Puppeteer over Selenium to generate load, we were able to test 4 times as many users for the same price.</p>
        <br/>
        <h4>Optimizing the Data Pipeline</h4>
        <figure>
          <img src="assets/images/4-design_decisions/elt_defs.svg">
        </figure>
        <p>Because simulating virtual users and extracting session metrics generates so much data, Monsoon naturally uses a pipeline to move data from one point to another in our system. Two common data pipeline patterns are ELT and ETL. The ‘E’ stands for Extract, the ‘L’ stands for Load and ‘T’ stands for Transform.</p>
        <p>The choice of one pattern over the other has significant impact on scalability.</p>
        <br/>
        <h5>An ELT Pipeline</h5>
        <figure>
          <img src="assets/images/4-design_decisions/elt_diag.svg">
          <figcaption>Fig. 4.2: An ELT data pipeline’s tradeoffs</figcaption>
        </figure>
        <br/>
        <p>ELT is the current industry standard. With ELT, we load all the raw data into storage before making any changes to that data. Benefits to this approach include simplicity and flexibility. Because all the data is stored together, all transformations can be done in one place. And since you hang on to all the raw data, you're free to run further analyses and change the data's final form at any point in the future.​</p>
        <p>But ELT has drawbacks as well. You need to transmit all data from the extraction point into storage. This can result in substantial bandwidth costs and transmission times. Furthermore, once the data is loaded into storage, you'll need to pay ongoing storage fees.</p>
        <br/>
        <h5>An ETL Pipeline</h5>
        <figure>
          <img src="assets/images/4-design_decisions/etl_diag.svg">
          <figcaption>Fig. 4.3: An ETL data pipeline’s tradeoffs</figcaption>
        </figure>
        <br/>
        <p>ETL is also an industry standard. With ETL, we transform the data between the extraction and storage points, so that we're only storing data that's already been transformed. Benefits to this approach include reduced transmission times and costs. You reduce the volume of data right after the extraction step, thus reducing the time and bandwidth costs associated with moving that data into storage. And since you're not storing your entire raw data set, storage costs are naturally reduced.​</p>
        <p>But there are cons to ETL. The first is transformation complexity. Your data may require transformations at multiple steps along the data pipeline. This can become technically complex. The ETL pattern also lacks flexibility. Since we're only storing transformed data, we effectively lock ourselves into a final form of the data before starting the pipeline.</p>
        <p>Another concern with ETL is data integrity. If something goes wrong in the pipeline, causing you to lose all or part of the transformed data, you don't have raw data to fall back on. The data would need to be regenerated, which may or may not be possible.</p>
        <br/>
        <figure>
          <img src="assets/images/4-design_decisions/etl_overview.svg">
          <figcaption>Fig. 4.4: Monsoon is an ETL pipeline</figcaption>
        </figure>
        <br/>
        <p>Ultimately we found the ETL pattern a better fit for our use case. We were willing to deal with the transformation complexity, and final data shape lock-in actually wasn't a problem for us. Every load test Monsoon runs shows the same metrics, so the final shape of the data never changes.</p>
        <br/>
        <figure>
          <img src="assets/images/4-design_decisions/etl_table.svg">
          <figcaption>Fig. 4.5: ETL is a better fit for Monsoon</figcaption>
        </figure>
        <br />
        <p>Let's quantify this difference by looking at database writes. Say we run two different load tests - each has 4000 virtual users and has the same duration and test script. If Monsoon's data pipeline were to follow the ELT pattern, the number of writes required increases by a factor of about 800. And to generate the same final graph in the local Weather Channel dashboard, 2 megabytes of information in an ETL pipeline would need to be 24 gigabytes of information in an ELT pipeline.</p>
        <p>As we mentioned earlier, AWS uses a billing model of “only pay for what you use”. Therefore if we use less storage and transmit less data, it costs less. ​So holding the infrastructure budget constant, a user is able to run load tests that are many orders of magnitude larger since we designed Monsoon with an ETL data pipeline.​</p>
        </br>
        <h3>4.2 Near Real Time</h3>
        <br />
        <p>Now let's turn to the design decisions we made so that Monsoon can display results in near real time.​</p>
        <p>First let's clarify some terminology. <strong>Real time data</strong> is data that's collected, processed and analyzed on a continual basis. The resulting information should be available to the end user immediately after being generated.​</p>
        <p>And <strong>near real time</strong> is just real time, but with a delay introduced beforehand. Teams view a situation as it existed in the recent past rather than as it is right now. (Note that there isn't a precise cut off for what is or isn't considered near real time.)​</p>
        <br />
        <h4>Why Near Real Time?</h4>
        <br/>
        <p>Load tests can be really long. It's not uncommon to run a test for hours, days or even weeks. Because of this, Monsoon can handle tests of arbitrary length – the limiting factor is only the user's AWS budget.​</p>
        <p>But we don't want our users to have to wait until a test completes before they see results – we'd like them to see results as soon as possible.​</p>
        <p>This means that Monsoon needs to extract, transform and load data into the database on a regular basis so it can be queried and displayed on the locally-hosted dashboard.​</p>
        <br/>
        <h4>Near Real Time Pros and Cons</h4>
        <br/>
        <p>But building Monsoon as a near real time framework has pros and cons.</p>
        <figure>
          <img src="assets/images/4-design_decisions/near_real_time_webs.svg">
          <figcaption>Fig. 4.6: Monsoon’s near real time dashboard prevents this</figcaption>
        </figure>
        <br/>
        <p>On the pro side, engineers can monitor their tests in near real time, spotting issues as they come up rather than waiting until the entire test completes. Additionally, if the load test is important enough, it can become something of an event at the company running it. While researching our project, we actually came across stories of large teams reserving conference rooms, ordering food and watching the load test results roll in live.​</p>
        <p>But a major drawback to implementing a near real time data pipeline is the complexity. There are a lot of components, and it's technically challenging to coordinate all of them so as to avoid losing or double counting data points. We'll return to this complexity later.</p>
        <p>Now let's move on to examine Monsoon's architecture in greater detail.</p>
        <br/>
        <!--Section 5-->
        <h2>5 Monsoon's Architecture</h2>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide43.svg">
          <figcaption>Fig. 5.1: Monsoon’s load generation architecture</figcaption>
        </figure>
        <br/>
        <p>The purpose of our load generation architecture is to create a given number of virtual users, have them perform scripted actions on the target website or web app, and record metrics about those actions.​</p>
        <br/>
        <h3>Load Generation</h3>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide44.svg">
          <figcaption>Fig. 5.2: Monsoon’s load generation app and its three key components</figcaption>
        </figure>
        <br/>
        <p>At the core of our load generation architecture is a Node app. The app’s critical components are:​</p>
        <ul>
          <li>Weather Station</li>
          <li>A user script</li>
          <li>Runner.js</li>
        </ul>
        <br/>
        <h4>Weather Station</h4>
        <br/>
        <p>Engineers can use the Weather Station library to write their test script. Weather Station makes use of methods from the Performance API built into browsers and gives engineers a way to write code to measure how long individual user actions take.​</p>
        <br/>
        <h4>Test Script</h4>
        <br/>
        <p>Our next load generation component is the user's test script. It's a set of instructions describing the exact actions the headless browser instances should take and in what order. Combined with Puppeteer, this test script is what allows Monsoon to program a virtual user.</p>
        <p>For example, returning to the engineers at Boost Health, their user test script might instruct the headless browser instance to load the main Boost Health webpage, wait two seconds, load a product details page, wait 10 seconds, then click the "Add to Cart" button.</p>
        <br/>
        <h4>Runner</h4>
        <br/>
        <p>Runner.js is a coordination script. It creates 20 headless Chrome instances controlled by Puppeteer to simulate 20 virtual users. Runner.js then instructs each virtual user to execute the commands listed in the user's test script and stores the resulting session metrics for each of those virtual user actions.​</p>
        <p>To summarize, our Node app creates virtual users, instructs them to perform actions, and saves metric data about the actions taken.​</p>
        <p>But how do we actually run the app? We use AWS ECS and Fargate.</p>
        <br/>
        <h4>Fargate and ECS</h4>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide49.svg">
          <figcaption>Fig. 5.3: Monsoon’s load generation app runs on Fargate and uses ECS</figcaption>
        </figure>
        <br/>
        <p>AWS Fargate is a serverless, pay-as-you-go compute engine that allows you to focus on building applications without managing servers. In effect, it's an AWS service that runs containerized applications.</p>
        <p>To run our app and scale to more than 20 users, we containerized our Node app and passed it to ECS, the Elastic Container Service. ECS is used for container orchestration.​</p>
        <p>For every block of 20 virtual users we wish to simulate, ECS spins up an additional Fargate instance to execute our containerized Node app.​ For example, to simulate 4000 users, ECS spins up 200 Fargate instances, each of which each of which executes one instance of our containerized app.​</p>
        <p>So now that our load generation process has yielded session data for our virtual users, what do we do with it?</p>
        <br/>
        <h3>Transformation</h3>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide51.svg">
          <figcaption>Fig. 5.4: Monsoon’s transformation architecture</figcaption>
        </figure>
        <br/>
        <p>To answer this, let's take a closer look at our Transformation architecture. High-level, the Transformation architecture takes the session metric data stored on each Fargate instance, transforms it into a time series data format, then loads it into a time series database.​</p>
        <p>A key piece of the Transformation component is our normalization logic. Before we dive into the details, let's clarify our definition of "normalization" since it can be a slippery term in the data science world.</p>
        <p><strong>What is “normalization” exactly?</strong></p>
        <p>We consider it a pre-processing step that facilitates data aggregation. When we normalize raw data, we're taking multiple data points with different raw timestamps and combining them into a single data point with one normalized timestamp. If you think of calculating a moving average, that's a good mental model.​</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/arch_normalizer_process.gif">
          <figcaption>Fig. 5.5: Think of “normalization” as calculating a moving average</figcaption>
        </figure>
        <br/>
        <h4>Normalizer.js</h4>
        <br/>
        <p>Earlier we mentioned that the runner.js file in our Node app stores session metrics from every virtual user action. We're storing these metrics by temporarily saving them to the local filesystems of the Fargate instances.</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide52.svg">
          <figcaption>Fig. 5.6: Monsoon’s Normalizer.js script transforms metrics and moves them to an S3 bucket</figcaption>
        </figure>
        <br/>
        <p>Alongside our load generation scripts, the Node app also runs a file called Normalizer.js. Normalizer.js polls the Fargate instance's local filesystem for new session metric data every 15 seconds. The Normalizer.js code applies transformations to any data it finds, then moves the transformed data to an S3 bucket for more permanent storage. This normalization step is necessary because the aggregation step that follows requires uniform timestamps.​</p>
        <p>So the core logic of Normalizer.js groups the session metric data by time window. And a happy side effect of our normalization process is that we preserve statistical significance while minimizing data size.​</p>
        <br/>
        <h4>S3 Bucket</h4>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide54.svg">
          <figcaption>Fig. 5.7: Monsoon’s S3 bucket stores normalized test results data</figcaption>
        </figure>
        <br/>
        <p>Now let's examine our destination S3 bucket more closely.</p>
        <p>Once metrics sent by Normalizer.js start hitting the S3 bucket, the S3 bucket will contain files organized based on timestamp, end user action and Fargate instance name. For a given end user action, a single file represents the metrics for all virtual users on one Fargate instance within a single 15-second time window.</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide55.svg">
          <figcaption>Fig. 5.8: Structure of one test result file</figcaption>
        </figure>
        <br/>
        <p>For example, in the figure above, the JSON file contains metrics from all 20 virtual users on the "mi_W8ni" Fargate instance for the "Click first link" action at the Unix timestamp listed.​</p>
        <p>But even in this transformed state, the data currently sitting in the S3 bucket isn't appropriately formatted for a time series database. So we'll need another transformation step, this time involving an AWS Lambda function.​</p>
        <br/>
        <h4>Aggregating Lambda</h4>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide56.svg">
          <figcaption>Fig. 5.9: Monsoon’s Aggregating Lambda</figcaption>
        </figure>
        <br/>
        <p>AWS Lambda is a serverless, event-driven compute service. It's similar to Fargate. But while Fargate runs entire apps in a serverless environment, Lambda runs individual serverless functions.​</p>
        <p>Every three minutes, our Aggregating Lambda function polls the S3 bucket to determine if any new data has arrived. For each new timestamp, the lambda aggregates all the metrics in the JSON files with that timestamp, creating a single time series data point. Then we POST that data point to the time series database.</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/aggregation_process.gif">
          <figcaption>Fig. 5.10: How metrics are aggregated by timestamp</figcaption>
        </figure>
        <br/>
        <h3>Storage</h3>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide58.svg">
          <figcaption>Fig. 5.11: Monsoon’s Storage Component</figcaption>
        </figure>
        <br/>
        <p>Now let's examine the Storage component of Monsoon's architecture. Its purpose is to appropriately store the data generated during the Transformation step.</p>
        <p>As with any system, this data should be stored in a format that lends itself to the data's shape and access patterns. In Monsoon's case, a good fit is the time series data format.​</p>
        <br/>
        <h4>What is Time Series Data?</h4>
        <br/>
        <p>A <strong>time series</strong> is a series of data points that are ordered chronologically. They're excellent at tracking change over time. Examples include stock charts, ocean tidal charts and line charts in Excel.​</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide59.svg">
          <figcaption>Fig. 5.12: Time series data tracks change over time</figcaption>
        </figure>
        <br/>
        <p>With Monsoon, we generate synthetic load on a website and track session metrics as the test progresses. End users are looking to track how key metrics like page load response times change over time. This type of data naturally lends itself to the time series format.</p>
        <br/>
        <h4>Which Time Series Database?</h4>
        <br/>
        <p>There are many time series database options available, but we chose to go with AWS Timestream. Timestream is a fast and scalable serverless time series database service that makes it easy to store and analyze trillions of events per day.​ Timestream can be up to 1000 times faster than a relational database and as little as 10% of the cost.​</p>
        <p>Timestream is a database service, therefore users don't host Timestream themselves. Instead, they connect to the Timestream service, then create tables within the service. Each table contains one or more related time series.</p>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide61.svg">
          <figcaption>Fig. 5.13: Records in Monsoon’s Timestream database</figcaption>
        </figure>
        <br/>
        <p>As we mentioned in the Transformation section, data in our S3 bucket is aggregated by timestamp, then converted into a time series data format and written to Timestream. Because time series data is a first-class citizen in Timestream, the database can quickly and efficiently read and write time series data. An added benefit is that the data returned by a Timestream query is already in the shape we need to eventually display it.</p>
        <br/>
        <h3>Visualization</h3>
        <br/>
        <figure>
          <img src="assets/images/5-architecture/slide62.svg">
          <figcaption>Fig. 5.14: Monsoon’s Visualization Component</figcaption>
        </figure>
        <br/>
        <p>Visualization is Monsoon's final architectural component. The Visualization architecture queries the Storage layer, then displays the data it retrieves in a graphical user interface.</p>
        <p>We call our dashboard Weather Channel. It's a locally-hosted React app that directly queries the Timestream database service. This dashboard app uses the Victory library for modular charting and data visualization, as Victory has native support for time series data. We’ll show Weather Channel in action later in this case study.</p>
        <br/>
        <!--Section 6-->
        <h2>6 Installing and Using Monsoon</h2>
        <br />
        <p>
          We've published Monsoon as an npm package, so you can just run <code>npm install –g monsoon-load-testing</code> to install Monsoon globally on your local machine. 
          After you've installed the package, you'll want to verify that your hidden AWS config file is correct and does not contain a default profile.
        </p>
        <figure>
          <img src="assets/images/6-install_use_monsoon/credentials.png" class="case-study-image">
          <figcaption>Fig 6.1: Verify your hidden AWS configuration files don’t contain default profiles.</figcaption>
        </figure>
        <br />
        <p>
          Next you can issue the <code>monsoon init</code> command. The <code>init</code> command also prompts you to enter your AWS credentials so that Monsoon can deploy infrastructure within your AWS account. 
          This creates a new <code>monsoon_tests</code> subdirectory inside your current working directory. You'll need to <code>cd</code> into <code>monsoon_tests</code>, and be sure to execute all of the other Monsoon CLI subcommands, from within <code>monsoon_tests</code>.
        </p>
        <figure>
          <img src="assets/images/6-install_use_monsoon/init.gif" class="case-study-image">
          <figcaption>Fig 6.2: The <code>monsoon init</code> command.</figcaption>
        </figure>
        <br />
        <p>      
          The next step is to issue the <code>monsoon deploy</code> command. This spins up the AWS ​
          infrastructure required to run your load test, including a custom VPC, multiple Lambda ​
          functions, an S3 bucket and a Timestream database.
        </p>
        <figure>
          <img src="assets/images/6-install_use_monsoon/deploy.gif" class="case-study-image">
          <figcaption>Fig 6.3: The <code>monsoon deploy</code> command.</figcaption>
        </figure>
        <br />
        <p>
          From within your <code>monsoon_tests</code> directory, you save your test script in the ​
          <code>test_script.js</code> file and configure your upcoming load test by modifying the <code>test_config.json</code> ​
          file. If you anticipate running multiple different load tests with different test scripts and ​
          configurations, you can set up additional subdirectories to organize all the different tests. Just ​
          issue the <code>monsoon new-test</code> command.
        </p>
        <br />
        <figure>
          <img src="assets/images/6-install_use_monsoon/newtest.gif" class="case-study-image">
          <figcaption>Fig 6.4: The <code>monsoon new-test</code> command.</figcaption>
        </figure>
        <br />
        <p>
          Let's also look at an example test script and its configuration. 
          Below we have a test script that loads the Boost homepage, then goes to the product page. 
          The test is configured to last one hour. We start out with 20 concurrent users, then ramp up to 4000 concurrent users over a 50-minute time window.
        </p>    
        <figure>
          <img src="assets/images/6-install_use_monsoon/script.svg" class="case-study-image">
          <figcaption>Fig 6.5: An example of test script.</figcaption>
        </figure>
        <br />
        <figure>
          <img src="assets/images/6-install_use_monsoon/config.svg" class="case-study-image">
          <figcaption>Fig 6.6: An example of configuration file.</figcaption>
        </figure>
        <br /> 
        <p>
          With the test script and configuration read to go, you’ll next run the <code>monsoon start</code> command. 
          This officially kicks off your load test and provides a bit of light reading while you wait.
        </p>
        <figure>
          <img src="assets/images/6-install_use_monsoon/start.gif" class="case-study-image">
          <figcaption>Fig 6.7: The <code>monsoon start</code> command.</figcaption>
        </figure>
        <br /> 
        <p>
          After about 7 minutes, your load test results will start streaming into the Weather Channel ​
          dashboard. You can view these results locally by issuing the <code>monsoon weather-channel</code> ​
          command, then opening up a browser and visiting localhost port 5000.
        </p>
        <br />
        <figure>
          <img src="assets/images/6-install_use_monsoon/weather.gif" class="case-study-image">
          <figcaption>Fig 6.8: The <code>monsoon weather-channel</code> command.</figcaption>
        </figure>
        <br />
        <p>
          Within the dashboard, you'll receive your data in near real time, so you can refresh your ​
          browser to see the most current results. Clicking on the buttons at the top of the screen, you​
          can choose which end user action you'd like to see results for. And you can also toggle​
          particular metrics on or off to see only the results that interest you.
        </p>
        <br />
        <p> 
          Weather Channel also allows you to zoom in and zoom out to get different perspectives on ​
          your results. And if you hover over a line, you can see your metrics at that exact point in ​
          time
        </p>
        <br />
        <figure>
          <img src="assets/images/6-install_use_monsoon/dashboard_demo.gif" class="case-study">
          <figcaption>Fig 6.9: A demo of monsoon dashboard <code>weather-channel</code>.</figcaption>
        </figure>
        <br />
        <p>
          Above we see what happened as we ramped up load on the Boost Health website to 4000 concurrent users. The results aren't good. 
          Response times have increased all the way to 6000 milliseconds. 
          Analyzing these results, the Boost engineers determine that their current infrastructure won't cut it for their upcoming product launch. 
          Without running this load test, the Boost engineers wouldn't have known about this weakness in their production system.
        </p>
        <br />
        <figure>
          <img src="assets/images/6-install_use_monsoon/teardown.gif" class="case-study-image">
          <figcaption>Fig 6.10: The <code>monsoon teardown</code> and <code>monsoon destroy</code> commands.</figcaption>
        </figure>
        <br />
        <p>
          Returning to our demo, once your test is over and you've viewed your results, 
          you can tear down your infrastructure by issuing the <code>monsoon teardown</code> command. 
          If you'd also like to delete all directories and environment files related to Monsoon, 
          run the <code>monsoon destroy</code> command after the <code>teardown</code> command.
        </p>
        <br />
        <!--Section 7-->
        <h2>7 Implementation Challenges</h2>
        <br />
        <p>
          Building a distributed load testing tool is a complex undertaking, and we faced several implementation challenges along the way. 
          Two categories of challenges stand out in particular.
        </p>
        </br>
        <h3>7.1 Response Time</h3>
        <p>
          Early in the process of designing Monsoon, we spent a lot ​of time considering exactly what metrics to track for a load test. 
          There were all sorts of things ​we could measure, 
          but we wanted to choose the one that engineers would find most useful.
        </p>
        <br />
        <p>
          Examining existing load testing tools, we found that all of them focus on response time as ​
          their core metric. At first glance, this seems reasonable, but digging deeper, we found ​
          "response time" to be a very fuzzy term. Maybe it’s the elapsed time between when a ​
          client issues a request and when that client receives the first byte of the response. Or maybe ​
          we keep the timer going until the DOM content is fully loaded. We even contacted customer ​
          support at Flood to get details about how they calculate response time, and we still weren’t ​
          able to get complete clarity.​
        </p>
        <br />
        <figure>
          <img src="assets/images/7-challenges/response_time.svg" class="case-study">
          <figcaption>Fig 7.1: Which definition of “response time” is most useful to engineers?</figcaption>
        </figure>
        <br />
        <p>
          Because the core unit of a test script is one end user action, we ultimately chose to define response time ​as the difference between the starting and ending times for the given end user action. 
          To ​make it easy for developers to calculate response times in their load tests, we wrote a library ​called Monsoon Weather Station, 
          as we mentioned earlier in the Architecture section.​
        </p>
        <br />
        <h3>7.2 Timing</h3>
        <p>Our other implementation challenges related to timing. These were by far the larger and more complex issues. </p>
        <br />

        <h4>Containers</h4>
        <p>
          When a developer using Monsoon issues the <code>monsoon start</code> command to kick off a load test, the CLI code sends an event to a starting Lambda, which in turn spins up the appropriate number of containers. 
          But it can take anywhere from 15 to 60 seconds for a container on Fargate to be ready to run. 
          Since load generation happens inside the containers, if the containers aren't all starting together, 
          then we're no longer truly testing the specified number of concurrent users.​
        </p>
        <br />
        <figure>
          <img src="assets/images/7-challenges/waiting_time.gif" class="case-study">
          <figcaption>Fig 7.2: A waiting time ensures all virtual users hit the website at once.</figcaption>
        </figure>
        <br />
        <p>
          To solve this problem, we introduced a waiting time of 3 minutes to make sure that all containers are in a ready state before Monsoon actually runs the test script. 
        </p>
        <br />

        <h4>Data Generation and Processing</h4>
        <p>
          Once all the containers are up and running, how do we coordinate the timing of data ​
          generation and processing?
        </p>
        <br />
        <p>
          We had to carefully orchestrate our data pipeline to solve this second timing issue. As ​
          mentioned in the architecture section, inside each container, 
          a runner script is responsible for generating headless Chrome instances with Puppeteer and running the test ​script to generate the data. 
          That raw data needs to be normalized and then sent off to our S3 ​bucket. 
          But in order to present near real time results to developers, we can't wait until all the ​raw data is generated before starting to process it. 
          Therefore our normalization script polls for ​new raw data every 15 seconds.    
        </p>
        <br />
        <figure>
          <img src="assets/images/7-challenges/polling.gif" class="case-study">
          <figcaption>Fig 7.3: Polling lets data keep moving through our pipeline.</figcaption>
        </figure>
        <br />
        <h4>Moving Data to Timestream</h4>
        <p>
          Now that the normalizing script has moved the processed data into the S3 bucket, we run into ​
          a third timing issue. How will we get the data sitting in the S3 bucket into our Timestream ​
          database?​
        </p>
        <br />
        <p>
          We handled this by creating an EventBridge rule to invoke our Metronome Lambda once every ​
          minute. The Metronome Lambda checks a collection of pre-generated timestamps to ​
          see if any have expired. If any have, the Metronome Lambda invokes the Aggregating Lambda, which gathers the new batch of data and sends it to Timestream.​
        </p>
        <br />
        <figure>
          <img src="assets/images/7-challenges/timestream.gif" class="case-study">
          <figcaption>Fig 7.4: A new Lambda ensures we move data from S3 to Timestream in a timely fashion.</figcaption>
        </figure>
        <p>
          After solving this third timing issue, we had a working data pipeline capable of moving raw test ​
          result data from containers to an S3 bucket and then to the database, performing transformations along the way.     
        </p>
        <br />
        <!-- Section 8 -->
        <h2>8 Future work</h2>
        <p>
          First, we'd like to <strong>rewrite certain parts of the project in Go</strong>. In its current state, Monsoon is written entirely in JavaScript and TypeScript. 
          This certainly works, but there are parts of the project, particularly the normalizing script and the Aggregating Lambda, that do a lot of heavy lifting in terms of data processing. 
          These pieces could benefit from the faster runtime performance and better CPU utilization of a compiled language like Go.
        </p>
        <br />
        <p>
          Second, we'd like to <strong>provide additional libraries</strong> so that we can support test scripts written for multiple load testing tools. For example, 
          we could provide a version of the Weather Station library that's compatible with Playwright test scripts.
        </p>
        <br />
        <p>
          Third, we'd like to implement <strong>a new feature that checks test scripts for correctness before ​
          actually running them</strong>. As it stands now, a developer can start a load test even if the test script ​
          itself is not written according to our instructions. This can waste valuable developer time while ​
          also incurring AWS bills for the infrastructure used. This pre-check would be incorporated behind the scenes as part of the <code>monsoon start</code> command.
        </p>
        <br />
        <p>
          And a final feature we'd like to implement is allowing developers to <strong>export their load test results to a CSV file</strong>.
        </p>
        <br />
        <!-- Section 9 -->
        <h2>9 References</h2>

        <p><strong>Modern Data Pipeline:</strong></p>
        <ul>
          <li><a href="https://hevodata.com/blog/cloud-data-warehouse-101/" class="references">History of the Cloud Data
              Warehouse</a></li>
          <li><a href="https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/"
              class="references">Emerging Architectures for Modern Data Infrastructure</a></li>
          <li><a href="https://rudderstack.com/blog/the-complete-customer-data-stack" class="references">The Complete
              Customer Data Stack</a></li>
          <li><a href="https://dataled.academy/guides/modern-data-stack-for-growth/" class="references">Modern Data
              Stack for Growth</a></li>
        </ul>
        <p><strong>Data Silos:</strong></p>
        <ul>
          <li><a
              href="https://rudderstack.com/blog/heres-why-the-cloud-tools-used-by-marketing-sales-and-product-create-data-silos"
              class="references">Why Cloud Tools Create Data Silos</a></li>
        </ul>
        <p><strong>Reverse ETL/Syncing:</strong></p>
        <ul>
          <li><a href="https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb" class="references">Reverse ETL,
              A Primer</a></li>
          <li><a href="https://www.grouparoo.com/solutions/reverse-etl" class="references">Reverse ETL</a></li>
          <li><a href="https://hightouch.io/customers/zeplin/" class="references">Reverse ETL Case Study</a></li>
        </ul>
        <p><strong>Companies:</strong></p>
        <ul>
          <li><a href="https://airbyte.io/" class="references">Airbyte</a></li>
          <li><a href="https://www.grouparoo.com/" class="references">Grouparoo</a></li>
          <li><a href="https://www.snowflake.com/" class="references">Snowflake</a></li>
          <li> <a href="https://www.getdbt.com/" class="references">DBT</a></li>
          <li><a href="https://rudderstack.com/" class="references">Rudderstack</a></li>
        </ul>
        <br>
        <h2>Presentation</h2>
        <div class="presentation-wrapper case-study-image">

          <iframe class="presentation-video" frameborder="0" src="https://www.youtube.com/embed/nQdf8djNlFA"
            allowfullscreen></iframe>
        </div class="presentation-wrapper">
        <br>
        <h2>Team</h2>
        <br />
        <div class="section team-section">
          <div class="container">
            <div data-duration-in="300" data-duration-out="100" class="tabs w-tabs">
              <div data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a"
                style="transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1) rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg); transform-style: preserve-3d; opacity: 0;"
                class="tabs-content w-tab-content">
                <div>
                  <div class="team-grid">
                    <div class="team-member-wrap">
                      <img src="assets/images/team/katherine.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Katherine Beck</div>
                        <div class="team-member-location">Los Angeles, CA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:hello@katherinebeck.me" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="http://katherinebeck.me" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/katherine-m-beck/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/minh.jpg" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Minh Vu</div>
                        <div class="team-member-location">Chicago, IL</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:minhphanh.vu@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://minhphanhvu.github.io/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/minh-vu-954223112/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/stephaniev1.jpeg" id="sc-photo-case-study" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Stephanie Cunnane</div>
                        <div class="team-member-location">Los Angeles, CA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:stephanie.cunnane@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://stephaniecunnane.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/stephaniecunnane" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/alex.jpeg" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Alex Drover</div>
                        <div class="team-member-location">Vancouver, BC</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:alexfdrover@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://alexfdrover.github.io" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/alexfdrover/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <br>
        <br>
    </article>
  </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
    type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
  <script src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
    type="text/javascript"></script>
  <script>
    /*!
     * toc - jQuery Table of Contents Plugin
     * v0.3.2
     * http://projects.jga.me/toc/
     * copyright Greg Allen 2014
     * MIT License
    */
    !function (a) { a.fn.smoothScroller = function (b) { b = a.extend({}, a.fn.smoothScroller.defaults, b); var c = a(this); return a(b.scrollEl).animate({ scrollTop: c.offset().top - a(b.scrollEl).offset().top - b.offset }, b.speed, b.ease, function () { var a = c.attr("id"); a.length && (history.pushState ? history.pushState(null, null, "#" + a) : document.location.hash = a), c.trigger("smoothScrollerComplete") }), this }, a.fn.smoothScroller.defaults = { speed: 400, ease: "swing", scrollEl: "body,html", offset: 0 }, a("body").on("click", "[data-smoothscroller]", function (b) { b.preventDefault(); var c = a(this).attr("href"); 0 === c.indexOf("#") && a(c).smoothScroller() }) }(jQuery), function (a) { var b = {}; a.fn.toc = function (b) { var c, d = this, e = a.extend({}, jQuery.fn.toc.defaults, b), f = a(e.container), g = a(e.selectors, f), h = [], i = e.activeClass, j = function (b, c) { if (e.smoothScrolling && "function" == typeof e.smoothScrolling) { b.preventDefault(); var f = a(b.target).attr("href"); e.smoothScrolling(f, e, c) } a("li", d).removeClass(i), a(b.target).parent().addClass(i) }, k = function () { c && clearTimeout(c), c = setTimeout(function () { for (var b, c = a(window).scrollTop(), f = Number.MAX_VALUE, g = 0, j = 0, k = h.length; k > j; j++) { var l = Math.abs(h[j] - c); f > l && (g = j, f = l) } a("li", d).removeClass(i), b = a("li:eq(" + g + ")", d).addClass(i), e.onHighlight(b) }, 50) }; return e.highlightOnScroll && (a(window).bind("scroll", k), k()), this.each(function () { var b = a(this), c = a(e.listType); g.each(function (d, f) { var g = a(f); h.push(g.offset().top - e.highlightOffset); var i = e.anchorName(d, f, e.prefix); if (f.id !== i) { a("<span/>").attr("id", i).insertBefore(g) } var l = a("<a/>").text(e.headerText(d, f, g)).attr("href", "#" + i).bind("click", function (c) { a(window).unbind("scroll", k), j(c, function () { a(window).bind("scroll", k) }), b.trigger("selected", a(this).attr("href")) }), m = a("<li/>").addClass(e.itemClass(d, f, g, e.prefix)).append(l); c.append(m) }), b.html(c) }) }, jQuery.fn.toc.defaults = { container: "body", listType: "<ul/>", selectors: "h1,h2,h3", smoothScrolling: function (b, c, d) { a(b).smoothScroller({ offset: c.scrollToOffset }).on("smoothScrollerComplete", function () { d() }) }, scrollToOffset: 0, prefix: "toc", activeClass: "toc-active", onHighlight: function () { }, highlightOnScroll: !0, highlightOffset: 100, anchorName: function (c, d, e) { if (d.id.length) return d.id; var f = a(d).text().replace(/[^a-z0-9]/gi, " ").replace(/\s+/g, "-").toLowerCase(); if (b[f]) { for (var g = 2; b[f + g];)g++; f = f + "-" + g } return b[f] = !0, e + "-" + f }, headerText: function (a, b, c) { return c.text() }, itemClass: function (a, b, c, d) { return d + "-" + c[0].tagName.toLowerCase() } } }(jQuery);
  </script>
  <script>
    /* initialize */
    $('.toc').toc({
      'selectors': 'h2', //elements to use as headings
      'container': 'article', //element to find all selectors in
      'smoothScrolling': true, //enable or disable smooth scrolling on click
      'prefix': 'toc', //prefix for anchor tags and class names
      'highlightOnScroll': true, //add class to heading that is currently in focus
      'highlightOffset': 100, //offset to trigger the next headline
    });
  </script>
</body>

</html>