<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">

<head>
  <meta charset="utf-8" />
  <title>Tapestry • Case Study</title>
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:regular,500,600,700" media="all" />
  <script type="text/javascript">
    WebFont.load({ google: { families: ["Inter:regular,500,600,700"] } });
  </script>
  <script type="text/javascript">
    !(function (o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      (n.className += t + "js"),
        ("ontouchstart" in o ||
          (o.DocumentTouch && c instanceof DocumentTouch)) &&
        (n.className += t + "touch");
    })(window, document);
  </script>
  <link href="assets/images/tapestry_graphic_mono.svg" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/images/tapestry_graphic_mono.svg" rel="apple-touch-icon" />
  <script src="https://kit.fontawesome.com/d019875f94.js" crossorigin="anonymous"></script>
  <meta name="image" property="og:image" content="assets/images/tapestry_logo_color.svg" />
</head>

<body>
  <div class="navigation-wrap">
    <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
      <div class="navigation-container">
        <div class="navigation-left">
          <a href="/" aria-current="page" class="brand w-nav-brand w--current" aria-label="home">
            <img src="assets/images/tapestry_graphic_color.svg" alt="" class="template-logo">
          </a>
          <nav role="navigation" class="nav-menu w-nav-menu">
            <a href="/case-study" class="link-block w-inline-block">
              <div>Case Study</div>
            </a>
            <a href="/team" class="link-block w-inline-block">
              <div>The Team</div>
            </a>
          </nav>
        </div>
        <div class="navigation-right">
          <div class="login-buttons">
            <a href="https://github.com/tapestry-pipeline" target="_blank">
              <span style="color: #161d6f">
                <i class="fab fa-github fa-lg"></i>
              </span>
            </a>
          </div>
        </div>
      </div>
      <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
    </div>
  </div>
  <div id="sidebar" class="toc">
  </div>
  <div class="section header">
    <article class="container case-study-container">
      <div class="hero-text-container">
        <h1 class="h1 centered">Case Study</h1>
      </div>
      <div id="case-study">
        <br />
        <br />
        <!-- Section 1 -->
        <h2 class="h2">1 Introduction</h2>
        <br>
        <p>
          Load tests are a critical tool for any company that wants to build and maintain robust, 
          scalable websites and web apps. They simulate traffic hitting your servers, 
          allowing you to measure exactly how your app performs under different load profiles.
        </p>
        <br>
        <p>
          The overall concept of load testing was born in the physical world. 
          For example, structural engineers load test bridges before opening them to the public and power engineers load test electrical distribution systems. 
        </p>
        <br>
        <p>
          But in the world of software, load testing is the process of directing simulated traffic at any application that handles requests, 
          be that a website, web app, API endpoint, even load balancers and caches, then measuring that software’s performance under the load.
        </p>
        <br />
        <h3>1.1 Who’s interested in load test results?</h3>
        <br />
        <p>
          Load test results are valuable to both business stakeholders and engineering teams. 
        </p>
        </br>
        <p>
          On the business side, when an app can’t cope with its current amount of traffic, the app’s performance degrades or the app may stop working entirely. 
          This results in an immediate drop in the company’s revenue.
        </p>
        <br />
        <p>
          From the engineering team’s perspective, load testing provides valuable feedback about your production system - how many concurrent users can it handle? 
          How does response time change as load increases? Is there an uptick in failed requests under heavier load?
        </p>
        <br />
        <p>
          It’s this engineering perspective that we’ll focus on in this case study of Monsoon.
        </p>
        <br />
        <h3>1.2 What is Monsoon?</h3>
        <br />
        <p>
          Monsoon is an open-source, serverless framework for running browser-based load tests in the cloud. 
        </p>
        <br />
        [TO INSERT: picture of Monsoon CLI and Weather Channel]
        [CAPTION: Fig 1.1: ]
        <br />
        <p>
          Monsoon allows software engineers to easily load test their single-page application in anticipation of traffic spikes or overall business growth. 
          Monsoon can simulate loads of up to 20,000 concurrent users, and tests can be of any duration, from minutes to weeks or longer. 
          Engineers can also see their load test results visualized in a near real time dashboard.
        </p>
        <br />
        <h3>1.3 Brain Boost, a Hypothetical User Story</h3>
        <br />
        <p>
          To see load testing in action, let’s spend some time with the engineering team at Boost Health. 
          Boost is a rapidly-growing startup in the health and wellness space.
        </p>
        <br />
        <p>
          The stakes are high. Boost’s marketing team has spent months planning the product launch for its new Brain Boost supplement. 
          It’s Boost’s biggest product launch ever, and business executives want to close new rounds of venture capital funding based on the success of Brain Boost. 
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/brain.svg" class="case-study-image" alt="">
          <figcaption>Fig 1.2: Going all in on Brain Boost.</figcaption>
        </figure>
        <br />
        <p>
          Boost expects more traffic than their site has ever seen on launch day. 
          They’re predicting peaks of 4,000 concurrent users. 
          The engineers at Boost are tasked with making sure the site can withstand all that traffic, so they decide to run load tests.
        </p>
        <br />
        <p>
          But before we discuss these load tests, it’s important to note that the Boost website is a single page application. 
          This has important implications for load testing.
        </p>
        <br />
        <h3>1.4 What is a Single Page Application?</h3>
        <br />
        <p>A single page application is a web app comprised of just a single HTML page. Unlike a traditional website, after the initial page load, there are no page reloads. 
          The content of the single page is just repeatedly updated on the fly using browser-side JavaScript.
        </p>
        <br />
        <p>
          SPAs have some significant advantages over more traditional websites:
        </p>
        <ul>
          <li>They reduce strain on server resources.</li>
          <li>They speed up development by decoupling the frontend code from the backend code.</li>
          <li>They provide a much more dynamic, responsive experience for the end user since there are no full page reloads.          </li>
        </ul>
        <br />
        <h3>1.5 Boost Health and Their Load Testing Journey</h3>
        <br />
        <p>
          So let’s follow along with the Boost engineers as they load test their SPA.
        </p>
        <br />
        <h4>First Attempt - Protocol-based load testing with JMeter</h4>

        <br />
        <p>
          One of the Boost engineers used a well-established, open-source load testing tool called Apache JMeter at a previous job, 
          so this is the first option the team tries.
        </p>
        <br />
        <p>
          JMeter is categorized as a protocol-based load testing tool. Protocol-based load tests are the original type of load test. 
          They involve traffic simulation at the HTTP protocol layer. 
          For example, if loading a webpage triggers HTTP requests for 75 subresources, with protocol-based testing, the developers will need to write code to request the original page AND all 75 of those subresources.
        </p>
        <br />
        <p>
          This is clearly a great deal of work for the team. On top of this, JMeter is a complex tool and the learning curve is steep. 
          The team settles in for many painful days of work ahead. 
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/unhappydev-jmeter.svg" class="case-study-image">
          <figcaption>Fig 1.3: Protocol-based load testing can be painful.</figcaption>
        </figure>
        <br />
        <p>
          After the team gets their first JMeter load test working, their results seem strangely incomplete. It turns out JMeter is a poor choice for load testing SPAs. 
          SPAs are JavaScript-intensive, but JMeter has no JavaScript interpreter and therefore can’t execute any JavaScript code. (The same holds true for other protocol-based load testing tools.) 
          Therefore, the bulk of an SPA is untestable with JMeter. This is a dealbreaker. 
        </p>
        <br />
        <h4>Second Attempt - Browser-based load testing with Selenium</h4>
        <br />
        <p>
          Another engineer on the Boost team knows of a different tool called Selenium and knows that Selenium can be used for browser-based load testing. 
        </p>
        <br />
        <p>
          <Strong>Browser-based load testing</strong> simulates web traffic using real web browsers rather than naked network requests. 
          Since we’re using browser instances to direct traffic to the site being tested, 
          those browsers clearly have built-in JavaScript interpreters and are fully capable of handling SPAs, unlike protocol-based load testing tools.
        </p>
        <br />
        <p>
          An even more fundamental difference between protocol-based load testing and browser-based load testing tools like Selenium is this: 
          Is the core unit we care about, is that the individual network request? Or is it an action the end user takes (which could actually result in 100 or more network requests)?
        </p>
        <br />
        <p>
          In Boost’s case, it’s an action a website user takes, like loading the homepage, 
          viewing product details or clicking the “Add to Cart” button.
        </p>
        <br />
        <p>
          Furthermore, thinking in terms of end user actions rather than lower-level network requests means browser-based load testing is significantly less complex than its protocol-based counterpart. 
          When planning how to test the Boost site, the engineers can think at a higher level of abstraction, which reduces bugs, makes for a better developer experience and saves significant amounts of developer time.
        </p>
        <br />
        <p>
          Returning to Selenium, it's a suite of browser-based test automation tools. 
          It was never actually designed for load testing, but that's how Selenium came to be used by many developers.
        </p>
        <br />
        <p> 
          The team runs their first browser-based load test using Selenium. 
          It’s a fairly small-scale test simulating 5 users concurrently visiting the Boost Health website. This test goes off without a hitch. 
        </p>
        <br />
        <p>
          Next, they test 100 concurrent users. This test doesn’t go so well. Selenium is a tool that runs locally on one of the engineers’ laptops, and testing 100 users means spinning up 100 browser instances. This is too resource-intensive for a single laptop. 
          And the actual number of users the Boost team needs to test is 4000, not 100. So the team has run out of local computing resources before they’re able to apply sufficient load to the Boost site.
        </p>
        <br />
        <figure>
          <img src="assets/images/1-introduction/unhappydev-selenium.svg" class="case-study-image">
          <figcaption>Fig 1.4: Local browser-based load testing doesn’t cut it.</figcaption>
        </figure>
        <br />
        <p>
          Clearly this is unworkable. Because browser-based load testing is so resource intensive, the Boost engineers need a solution that’s hosted in the cloud.
        </p>
        <br />
        <h4>Third Attempt - Browser-based load testing with Flood</h4>
        <br />
        <p>
          Researching cloud-hosted browser-based load testing, the team quickly discovers a platform called Flood. 
          Flood is an industry leader in the cloud-hosted, browser-based load testing space. Their platform is definitely capable of generating the 4000 concurrent users Boost needs to load test their site. However, Flood is very expensive. 
          We'll return to Flood later, but for now, the cost is a major drawback, enough to rule Flood out.
        </p>
        <figure>
          <img src="assets/images/1-introduction/unhappydev-flood.svg" class="case-study-image">
          <figcaption>Fig 1.5: Flood gets the job done but costs too much.</figcaption>
        </figure>
        <br />
        <h4>The Journey So Far</h4>
        <p>
          Let's summarize where the Boost engineers are right now. They initially tried protocol-based load testing. This was unsuccessful because protocol-based tools can't test an SPA's JavaScript code. 
          Next, they tried local browser-based load testing. 
          This too was unsuccessful because browser-based load testing is too resource intensive for a single machine. Third, they tried browser-based load testing in the cloud with the Flood platform. 
          However, this proved prohibitively expensive. 
        </p>
        <br />
        <p>
          So the team searches for a more economical option for browser-based load testing in the cloud. 
          In short order, they come across an open source tool called Monsoon.
        </p>
        <br />
        <!-- Section 2 -->
        <h2>2 Overview of Monsoon</h2>
        <br />
        <p>
          Monsoon is browser-based load testing platform hosted in the cloud. We built Monsoon to allow small- to medium-sized companies to load test their SPAs in a simple and scalable way, allowing them to get insights into its performance in near real time. ​
        </p>
        <br/>
        <h3>Four Key Components</h3>
        <p>
          Monsoon uses Amazon Web Services as its cloud provider, and all the Monsoon infrastructure is deployed directly into your AWS account. This infrastructure has 4 key components: Load Generation, Transformation, Storage and Visualization. 
        </p>
        <figure>
          <img src="assets/images/2-overview/monsoon_architecture_all.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.1: Monsoon's 4 key components</figcaption>
        </figure>
        <br />
        <h4>Load Generation</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_load_generation.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.2: Load Generation architecture</figcaption>
        </figure>
        <p>
          A​t the start of the load test, Monsoon provisions multiple compute instances to generate load. The number of instances scales linearly with the needs of the test. Currently Monsoon is able to handle up to 20,000 virtual users.
        </p>
        <br />
        <figure>
          <img src="assets/images/2-overview/cloud_purple.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.3: Headless Chrome instances in the cloud</figcaption>
        </figure>
        <p>
          Inside one of these compute instances, we use a tool called Puppeteer to simulate virtual users visiting the Boost Health website. Puppeteer is a powerful, browser-based testing library for Node, distributed as an npm package. Puppeteer provides us with a high-level API that gives us full control over headless Chrome instances.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/puppeteer_virtual_user.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.4: Monsoon uses Puppeteer to simulate users visiting a website</figcaption>
        </figure>
        <p>
          Puppeteer is very easy to use. For example, to simulate a user filling out a form, you just pass a CSS selector and the data as arguments to the `type` method. And to simulate a user clicking a button, just pass that button’s CSS selector to the `click` method.
        </p>
        <br/>
        <p>
          Thanks to Puppeteer, Monsoon is able to abstract away a lot of the complexity of the load testing process. With Monsoon, you no longer write code to hit specific backend API endpoints. Instead, you write a simple Puppeteer script that describes the end user actions you want to simulate. Then you configure the load test by specifying the number of simulated users and the duration of the test.
        </p>
        <br/>
        <p>
          Once we have the test script and its configuration, Monsoon uses Puppeteer to create and control headless Chrome instances. Each of those Chrome instances executes the actions defined by the Boost engineers in their test script, hitting dozens or hundreds of backend API endpoints in the process.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/puppeteer_load_testing.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.5: Simulated users execute the test script, and Monsoon records the metrics</figcaption>
        </figure>
        <p>
          Then Monsoon records performance metrics for each end user action and saves the data in JSON format. This process repeats for the duration of the load test.
        </p>
        <br/>

        <h4>Transformation</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_transformation.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.6: Transformation architecture</figcaption>
        </figure>
        <p>
          This load generation step can result in a serious amount of data. A one-hour test can easily generate hundreds of millions of data records. Let’s examine how Monsoon’s transformation architecture manages all this data.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/aggregation_pipeline.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.7: Monsoon aggregates and stores test results</figcaption>
        </figure>
        <p>
          Once we have our first raw test results ready, Monsoon performs a pre-processing step inside the compute instances. This readies the results to enter Monsoon's data pipeline, beginning the transformation step we see above. In this step, the test results for all virtual users are aggregated in increments of 15 seconds.​ These aggregated test results then need to be stored in a database.
        </p>
        <br/>

        <h4>Storage</h3>
        <br />
        <figure>
          <img src="assets/images/2-overview/architecture_storage.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.8: Storage Architecture</figcaption>
        </figure>
        <p>
          or our Storage component, we used a time-series database. This type of database is optimized for time-series data, which is simply data points that are ordered by timestamp. We'll talk more about time-series data and why we opted for a time-series database later in this case study.​
        </p>
        <br/>
        <h4>Visualization</h3>
        <figure>
          <img src="assets/images/2-overview/architecture_visualization.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.9: Monsoon's Weather Channel, a near real time dashboard</figcaption>
        </figure>
        <br/>
        <p>
          Test results stored in the database can be visualized using Monsoon’s Weather Channel. Weather Channel is a locally-hosted dashboard that lets you see how your site actually performed under load.
        </p>
        <br/>
        <figure>
          <img src="assets/images/2-overview/metrics.svg" class="case-study-image-large" alt="">
          <figcaption>Fig. 2.10: Weather Channel gives you insight into the performance of your app</figcaption>
        </figure>
        <br/>
        <p>
          Specifically, Weather Channel allows you to track:
        </p>
        <ul>
          <li>Response time</li>
          <li>Concurrent users </li>
          <li>Transaction rate (end user actions performed per minute)</li>
          <li>Pass ratio</li>
        </ul>
        <!-- Section 3-->
        <h2>3 Who should use Monsoon?</h2>
        <br />
        <p>
          Let's take a look at Boost's other browser-based load testing options.
        </p>
        <figure>
          <img src="assets/images/3-who_should_use_monsoon/3_1_SaaS_vs_DIY.gif" class="case-study-image-large" alt="">
          <figcaption>Fig. 3.1: Browser-based load testing options</figcaption>
        </figure>
        </br>
        <p>
          There are two typical paths Boost might choose. The first is a cloud-based software as a service solution, like the Flood platform mentioned earlier. The second is a do-it-yourself approach, tasking the Boost engineers with building their own browser-based load testing tool.​
        </p>
        </br>
        <p>
          Let's examine the tradeoffs for a cloud-based SaaS solution first. A platform like Flood is highly scalable, able to simulate tens or even hundreds of thousands of concurrent users. It's also generally very easy to use. It's usually a hosted platform, meaning the end user doesn't need to configure or deploy any infrastructure. This makes the overall process quite straightforward. However the downside is cost. A single test can cost thousands of dollars. And a small-to-medium sized company like Boost may have a difficult time justifying spending that much on yet another SaaS product.​
        </p>
        <br />
        <p>
          Turning to a DIY approach, a major benefit here is that, once the platform is built, operation is much cheaper. You'll only pay for the underlying cloud infrastructure, no ongoing platform fees. However, building a distributed load testing platform requires a lot of engineering hours. Those engineering hours are expensive and would be hard to justify for a company of Boost's size. Furthermore, Boost doesn't have a huge engineering team. To build a distributed load testing tool from scratch, they'll have to pull engineers off projects that are central to their core business.​
        </p>
        <br/>
        <p>
          Neither of these options seems like a good fit for Boost. But what if there were another option? That’s where we think Monsoon fits in.
        </p>
        <figure>
          <img src="assets/images/3-who_should_use_monsoon/3_2_Solutions_comparison_table.svg">
          <figcaption>Fig. 3.2: Monsoon's niche in the browser-based load testing space</figcaption>
        </figure>
        <br/>
        <p>
          Monsoon glues together all the resources needed to create a near real time load testing platform, then deploys it directly to Boost's AWS account. 
        </p>
        <br/>
        <p>
          Thanks to its serverless nature, Monsoon is just as easy to use as existing cloud-based SaaS solutions like Flood. Additionally, since Monsoon would be deployed directly to the Boost AWS account, Boost retains ownership of its data.​
        </p>
        <br/>
        <p>
          However, Monsoon is neither as scalable nor as feature rich as other cloud-based SaaS products or highly-customized DIY solutions. Monsoon is an open source solution intentionally optimized for small- to medium-sized companies that want to load test their SPA for up to 20,000 concurrent users.​
        </p>
        <br/>
        <p>
          The good news is that the Monsoon team has already invested the engineering hours so you don't have to. We also charge no platform fees. You only pay the AWS charges stemming from the infrastructure spun up to run your test. This makes Monsoon significantly more affordable than either cloud-based SaaS solutions or DIY approaches.​
        </p>
        <br/>
        <!-- Section 4-->
        <h2>4 Design Decisions</h2>
        <br />
        <p>
          Early on, we made two design decisions that strongly shaped the Monsoon framework. The first was choosing to focus on high scalability. The second was striving to provide near real time results to engineers using Monsoon.
        </p>
        </br>
        <h3>4.1 Scalability</h3>
        <p>We set out to build a framework able to simulate as many users as possible, for as long as possible, as cheaply as possible. To accomplish this, we focused on the scalability of the load generation engine and of the data pipeline.​</p>
        <br/>
        <h4>Optimizing the Load Generation Engine</h4>
        <figure>
          <img src="assets/images/4-design_decisions/engine_figure.svg">
          <figcaption>Fig. 4.1: How Puppeteer stacks up against Selenium</figcaption>
        </figure>
        <br/>
        <p>​We had several options for simulating virtual users via headless browser instances. Selenium is the industry default. One Selenium app instance is able to simulate 5 virtual users. We compared this to Google's Puppeteer discussed earlier. One Puppeteer app instance is able to simulate 20 virtual users.</p>
        <p>This difference in simulated users per app instance is hugely important. Since each app instance requires its own execution environment, increasing the number of simulated users per app instance reduces the compute resources you need. And since AWS's billing model is "only pay for what you use", using compute resources more efficiently directly leads to cost savings.</p>
        <p>By choosing Puppeteer over Selenium to generate load, we were able to test 4 times as many users for the same price.</p>
        <br/>
        <h4>Optimizing the Data Pipeline</h4>
        <figure>
          <img src="assets/images/4-design_decisions/elt_defs.svg">
        </figure>
        <p>Because simulating virtual users and extracting session metrics generates so much data, Monsoon naturally uses a pipeline to move data from one point to another in our system. Two common data pipeline patterns are ELT and ETL. The ‘E’ stands for Extract, the ‘L’ stands for Load and ‘T’ stands for Transform.</p>
        <p>The choice of one pattern over the other has significant impact on scalability.</p>
        <br/>
        <h5>An ELT Pipeline</h5>
        <figure>
          <img src="assets/images/4-design_decisions/elt_diag.svg">
          <figcaption>Fig. 4.2: An ELT data pipeline’s tradeoffs</figcaption>
        </figure>
        <br/>
        <p>ELT is the current industry standard. With ELT, we load all the raw data into storage before making any changes to that data. Benefits to this approach include simplicity and flexibility. Because all the data is stored together, all transformations can be done in one place. And since you hang on to all the raw data, you're free to run further analyses and change the data's final form at any point in the future.​</p>
        <p>But ELT has drawbacks as well. You need to transmit all data from the extraction point into storage. This can result in substantial bandwidth costs and transmission times. Furthermore, once the data is loaded into storage, you'll need to pay ongoing storage fees.</p>
        <br/>
        <h5>An ETL Pipeline</h5>
        <figure>
          <img src="assets/images/4-design_decisions/etl_diag.svg">
          <figcaption>Fig. 4.3: An ETL data pipeline’s tradeoffs</figcaption>
        </figure>
        <br/>
        <p>ETL is also an industry standard. With ETL, we transform the data between the extraction and storage points, so that we're only storing data that's already been transformed. Benefits to this approach include reduced transmission times and costs. You reduce the volume of data right after the extraction step, thus reducing the time and bandwidth costs associated with moving that data into storage. And since you're not storing your entire raw data set, storage costs are naturally reduced.​</p>
        <p>But there are cons to ETL. The first is transformation complexity. Your data may require transformations at multiple steps along the data pipeline. This can become technically complex. The ETL pattern also lacks flexibility. Since we're only storing transformed data, we effectively lock ourselves into a final form of the data before starting the pipeline.</p>
        <p>Another concern with ETL is data integrity. If something goes wrong in the pipeline, causing you to lose all or part of the transformed data, you don't have raw data to fall back on. The data would need to be regenerated, which may or may not be possible.</p>
        <br/>
        <figure>
          <img src="assets/images/4-design_decisions/etl_overview.svg">
          <figcaption>Fig. 4.4: Monsoon is an ETL pipeline</figcaption>
        </figure>
        <br/>
        <p>Ultimately we found the ETL pattern a better fit for our use case. We were willing to deal with the transformation complexity, and final data shape lock-in actually wasn't a problem for us. Every load test Monsoon runs shows the same metrics, so the final shape of the data never changes.</p>
        <br/>
        <figure>
          <img src="assets/images/4-design_decisions/etl_table.svg">
          <figcaption>Fig. 4.5: ETL is a better fit for Monsoon</figcaption>
        </figure>
        <br />
        <p>Let's quantify this difference by looking at database writes. Say we run two different load tests with the test script, virtual user count and duration held constant. If Monsoon's data pipeline were to follow the ELT pattern, the number of writes required increases by a factor of about 800. And to generate the same final graph in the local Weather Channel dashboard, 2 megabytes of information in an ETL pipeline would need to be 24 gigabytes of information in an ELT pipeline.</p>
        <p>As we mentioned earlier, AWS uses a billing model of “only pay for what you use”. Therefore if we use less storage and transmit less data, it costs less. ​So holding the infrastructure budget constant, a user is able to run load tests that are many orders of magnitude larger since we designed Monsoon with an ETL data pipeline.​</p>
        </br>
        <h3>4.2 Near Real Time</h3>
        <br />
        <p>Now let's turn to the design decisions we made so that Monsoon can display results in near real time.​</p>
        <p>First let's clarify some terminology. <strong>Real time data</strong> is data that's collected, processed and analyzed on a continual basis. The resulting information should be available to the end user immediately after being generated.​</p>
        <p>And <strong>near real time</strong> is just real time, but with a delay introduced beforehand. Teams view a situation as it existed in the recent past rather than as it is right now. (Note that there isn't a precise cut off for what is or isn't considered near real time.)​</p>
        <br />
        <h4>Why Near Real Time?</h4>
        <br/>
        <p>Load tests can be really long. It's not uncommon to run a test for hours, days or even weeks. Because of this, Monsoon can handle tests of arbitrary length – the limiting factor is only the user's AWS budget.​</p>
        <p>But we don't want our users to have to wait until a test completes before they see results – we'd like them to see results as soon as possible.​</p>
        <p>This means that Monsoon needs to extract, transform and load data into the database on a regular basis so it can be queried and displayed on the locally-hosted dashboard.​</p>
        <br/>
        <h4>Near Real Time Pros and Cons</h4>
        <br/>
        <p>But building Monsoon as a near real time framework has pros and cons.</p>
        <figure>
          <img src="assets/images/4-design_decisions/near_real_time_webs.svg">
          <figcaption>Fig. 4.6: Monsoon’s near real time dashboard prevents this</figcaption>
        </figure>
        <br/>
        <p>On the pro side, engineers can monitor their tests in near real time, spotting issues as they come up rather than waiting until the entire test completes. Additionally, if the load test is important enough, it can become something of an event at the company running it. While researching our project, we actually came across stories of large teams reserving conference rooms, ordering food and watching the load test results roll in live.​</p>
        <p>But a major drawback to implementing a near real time data pipeline is the complexity. There are a lot of components, and it's technically challenging to coordinate all of them so as to avoid losing or double counting data points. We'll return to this complexity later.</p>
        <p>Now let's move on to examine Monsoon's architecture in greater detail.</p>
        <br/>
        <h2>5 Monsoon's Architecture</h2>
        <!--Section 5-->
        <h2>5 Using Tapestry</h2>
        <br />
        <h3>5.1 Prerequisites & Installing Tapestry</h3>
        <p>
          Getting started with Tapestry is pretty simple. You will need the following:
        </p>
        <ul>
          <li>Node and NPM</li>
          <li>An AWS account and AWS CLI</li>
          <li>Docker</li>
        </ul>
        <br />
        <p>
          If you were the developer, you would first need Node and NPM installed since Tapestry is a Node package. Since
          Tapestry provisions several AWS resources, you are required to have an AWS account and the AWS Command Line
          Interface configured on your machine. Finally, you will need to have a Docker account and have it installed on
          your machine.
        </p>
        <br />
        <p>
          After these preliminary steps, all you would need to do to get started is run <code
            class="command">npm i -g tapestry-pipeline</code>, and a host of commands will be provided to you.
        </p>
        <br />
        <h3>5.2 Tapestry Commands</h3>
        <figure>
          <img src="assets/images/demo/26_commandlist_v2.png">
          <figcaption>Tapestry's list of commands.</figcaption>
        </figure>
        <br />
        <p>
          As a new user, the first Tapestry command you would run is <code class="command">tapestry init</code>.
        </p>
        <figure>
          <img src="assets/images/demo/27_tapestry_init_computer.png" class="case-study-image-small">
          <figcaption>Tapestry provides a CloudFormation template during the init command.</figcaption>
        </figure>
        <br />
        <p>
          With <code class="command">tapestry init</code>, you give your project a name, and Tapestry will provision a
          project folder along with an AWS CloudFormation template. This template allows you to provision and configure
          AWS resources with code. In particular, this template is used to provision resources for the data ingestion
          phase of the pipeline. What Tapestry provides for the syncing phase of your pipeline is dependent upon which
          command you run next.
        </p>
        <br />
        <figure>
          <img src="assets/images/demo/27_init_v2.gif">
          <figcaption>Running <em>tapestry init</em> from the command line.</figcaption>
        </figure>
        <h3>5.3 Deploy vs. Kickstart Commands</h3>
        <br />
        <p>
          Next, you have a choice between the <code class="command">tapestry deploy</code> or <code
            class="command">tapestry kickstart</code> commands. Once you make your selection, Tapestry provides all the
          necessary configuration files for the data syncing phase.
        </p>
        <figure>
          <img src="assets/images/demo/28_deploy_v_kickstart.png" class="case-study-image">
        </figure>
        <br />
        <p>
          Both commands automate the deployment of a fully operational pipeline, but <code
            class="command">kickstart</code> includes two pre-configured sources, Zoom and Salesforce, along with one
          destination, Mailchimp. These pre-configured third-party tools set up your pipeline to have immediate
          end-to-end data flow, beginning with data ingestion and ending with data syncing into these tools. Regardless
          of which command you choose, note that a Snowflake account is required for <em>both</em> <code
            class="command">deploy</code> and <code class="command">kickstart</code>.
        </p>
        <br />
        <h3>5.4 End-to-End Demo</h3>
        <br />
        <p>
          To better show the full flow of data through a Tapestry pipeline, this section will walk through our <code
            class="command">tapestry kickstart</code> command.
        </p>
        <figure>
          <img src="assets/images/demo/29_tapestry_kickstart_computer.png" class="case-study-image-small">
        </figure>
        <br />
        <p>
          Prior to execution, you will have to own or create accounts with Zoom, Salesforce, and Mailchimp. <code
            class="command">kickstart</code> then begins by prompting you with a short series of questions about the
          previously mentioned accounts, as well as Snowflake, Tapestry’s data warehouse of choice. Tapestry stores this
          information in the AWS SSM Parameter store. This keeps sensitive data safe, but also accessible.
        </p>
        <figure>
          <img src="assets/images/demo/29_kickstart_questions_stars.gif">
          <figcaption>Kickstart command prompts user for inputs.</figcaption>
        </figure>
        <br />
        <p>
          After your information has been collected, <code class="command">kickstart</code> continues by creating the
          necessary databases and tables within your data warehouse to be utilized by both your ingestion and syncing
          tools.
        </p>
        <br />
        <p>
          Let’s quickly review the infrastructure that this command is provisioning.
        </p>
        <figure>
          <img src="assets/images/demo/new_data_ingestion_v2.gif" class="large-image">
          <figcaption>Data ingestion stack created during deployment.</figcaption>
        </figure>
        <br />
        <p>
          Tapestry uses the CloudFormation template supplied during the <code class="command">init</code> command to
          create a CloudFormation stack, provisioning AWS resources specifically related to your ingestion tool,
          Airbyte. These resources include an S3 staging bucket, an EC2 instance for Airbyte to run on, and an
          Application Load Balancer to route traffic to our EC2 instance. Airbyte is then configured to extract certain
          data from your Zoom and Salesforce accounts and send it over to your warehouse.
        </p>
        <figure>
          <img src="assets/images/demo/MISC_kickstart_airbyte_v2.gif">
          <figcaption>Setup and provisioning of data ingestion stack from the command line.</figcaption>
        </figure>
        <br />
        <p>
          You will then be asked to carry out a few steps so the data is transformed in your warehouse using the data
          model Tapestry provides for DBT. The raw data will be aggregated from both sources into one transformed table,
          filtered for duplicates, and appropriately formatted to be synced to Mailchimp.
        </p>
        <br />
        <figure>
          <img src="assets/images/demo/dbt-run.PNG" class="case-study-image">
          <figcaption>A successful DBT run, transforming data in the warehouse.</figcaption>
        </figure>
        <br />
        <p>
          To complete the pipeline, <code class="command">kickstart</code> creates another CloudFormation stack, this
          time spinning up various AWS resources for your syncing tool, Grouparoo.
        </p>
        <figure>
          <img src="assets/images/demo/new_data_syncing.gif" class="large-image">
          <figcaption>Data syncing stack created during deployment.</figcaption>
        </figure>
        <br />
        <p>
          These resources include an Elastic Container Services cluster to run your Grouparoo application, an Elastic
          Container Registry repository with your Grouparoo Docker image stored, and another Application Load Balancer
          to route network traffic to your cluster.
        </p>
        <figure>
          <img src="assets/images/demo/kickstart-grouparoo_v3.gif">
          <figcaption>Setup and provisioning of data syncing stack from the command line.</figcaption>
        </figure>
        <br />
        <h3>5.5 Tapestry Dashboard</h3>
        <br />
        <p>
          If you are deploying a new pipeline, Tapestry automatically launches your very own local Tapestry Dashboard.
          Additionally, anytime you'd like to view the dashboard, you can run the command <code
            class="command">tapestry start-server</code> to spin up and launch the UI at http://localhost:7777.
        </p>
        <figure>
          <img src="assets/images/demo/tap_ui_home_v2.png">
        </figure>
        <br />
        <p>
          The dashboard contains documentation for how to use Tapestry, along with various pages for each section of
          your pipeline. Each page displays metrics that give you better insight into the health of each component. They
          also include links to the UIs for all your date pipeline tools: Airbyte, Snowflake, DBT, and Grouparoo.
        </p>
        <br />
        <figure>
          <img src="assets/images/demo/tap_ui_carousel_v2.gif">
        </figure>
        <br />
        <p>Some important metrics we track on the dashboard include:</p>
        <ul>
          <li>Number of data ingestion sources currently operational</li>
          <li>Number of data syncing destinations currently operational</li>
          <li>EC2 instance status</li>
          <li>ECS cluster status</li>
          <li>CPU utilization</li>
          <li>List of source tables in the data warehouse</li>
          <li>List of transformed tables created in the data warehouse</li>
          <li>Logs for each tool for increased observability</li>
        </ul>
        <br />
        <h3>5.6 Tapestry Data Flow</h3>
        <br />
        <p>
          Now let’s actually see data flow through the entire pipeline with an example using the <code
            class="command">tapestry kickstart</code> command.
        </p>
        <figure>
          <img src="assets/images/demo/32_data_flow.png" class="case-study-image">
          <figcaption>Flow of data with kickstart command.</figcaption>
        </figure>
        <br />
        <p>
          Our goal in this example is to extract data from both Zoom and Salesforce and push it to our data warehouse,
          Snowflake. From there, we want to combine a list of webinar registrants from Zoom and Salesforce contacts into
          one single table, and filter out any duplicates along the way. Finally, we want to send this complete set of
          data over to Mailchimp.
        </p>
        <br />
        <h4>Zoom Data</h4>
        <figure>
          <img src="assets/images/demo/33_zoom_source.png" class="case-study-image-small">
          <figcaption>Source data from Zoom.</figcaption>
        </figure>
        <br />
        <p>
          As you can see from this table, we have several different people who have registered for a Zoom webinar. Of
          particular note are Diana Prince, Barry Allen, and Betty Rubble. As indicated in blue, Betty Rubble is a
          unique contact only found in Zoom, while the purple around Diana and Barry indicates that they are contacts
          found in both Zoom and Salesforce. Our goal is to get all three of these entries over to Mailchimp, with only
          one entry each for Diana and Barry.
        </p>
        <br />
        <h4>Salesforce Data</h4>
        <figure>
          <img src="assets/images/demo/34_salesforce_source.png" class="large-image">
          <figcaption>Source data from Salesforce.</figcaption>
        </figure>
        <br />
        <p>
          Now in Salesforce, we have a different list of customers. In yellow, there is a contact who is unique to
          Salesforce, Jack Rogers, while Diana and Bartholomew, a.k.a. "Barry" are in purple again, as seen in the Zoom
          list.
        </p>
        <br />
        <p>
          Even though Barry is going by a different first name in Salesforce, his email is the same across both sources,
          allowing us to uniquely identify him. This means that when we combine the two lists, we can use the email
          address field as a unique key to eliminate record duplication.
        </p>
        <br />
        <h4>Snowflake Warehouse</h4>
        <figure>
          <img src="assets/images/demo/35_snowflake_warehouse.png">
          <figcaption>Source data and transformed data in the warehouse.</figcaption>
        </figure>
        <br />
        <p>
          Once the data from both Zoom and Salesforce have made it into our Snowflake warehouse, we can transform the
          data by combining both lists, and removing duplicate entries. You can see the two tables highlighted in
          purple. The TAPESTRY_WEBINAR_REGISTRANTS table has all of our Zoom data, and the TAPESTRY_CONTACT table has
          all of our Salesforce data. The EMAIL_MODEL table that is highlighted in blue is the newly transformed table
          that we will sync to our Mailchimp account.
        </p>
        <br />
        <h4>Mailchimp</h4>
        <figure>
          <img src="assets/images/demo/36_mailchimpdestination.png" class="large-image">
          <figcaption>Synced data in Mailchimp destination.</figcaption>
        </figure>
        <br />
        <p>
          Finally, here in Mailchimp, we see that we’ve successfully synced all of our Zoom webinar registrants and
          Salesforce contacts, and made sure that there is only one entry for Barry and Diana. You can also see that our
          unique users, Betty and Jack made it over as well.
        </p>
        <br />
        <h3>5.7 Maintenance & Management</h3>
        <br />
        <h4>Rebuild Command</h4>
        <br />
        <p>
          Tapestry also supplies users with a <code class="command">tapestry rebuild</code> command that is specific to
          the syncing side of the pipeline. While most updates to Airbyte can be done right in their UI, Grouparoo’s
          dashboard is mainly for application visibility and observability. In order to add, remove, or update any
          sources or destinations, changes need to be made to the configuration files in your local Grouparoo directory.
        </p>
        <br />
        <p>
          Once these changes are finalized locally, the image must be rebuilt, pushed to a private repository in the
          Elastic Container Registry, and the Grouparoo CloudFormation stack must be updated. All of these steps are
          what Tapestry’s <code class="command">rebuild</code> command automates. The user simply makes the changes
          themselves to the configuration files and then runs <code class="command">tapestry rebuild</code>. Tapestry
          handles the rest.
        </p>
        <figure>
          <img src="assets/images/demo/ECR_only.png" class="case-study-image-small">
          <figcaption>Updated Docker image is pushed to ECR.</figcaption>
        </figure>
        <br>
        <h4>Teardown Command</h4>
        <br>
        <p>
          If the user, for whatever reason, decides they are no longer in need of Tapestry’s data pipeline, Tapestry
          provides <code class="command">tapestry teardown</code> to terminate and remove most of the AWS resources
          provisioned during deployment, as well as the Airbyte and Grouparoo applications that ran on those resources.
        </p>
        <br />
        <p>
          We say “most” AWS resources because we do not destroy your S3 bucket, nor do we destory any parameters in your
          AWS SSM Parameter Store. Tapestry uses the Parameter Store to store user inputs, such as API keys and various
          account credentials. These resources remain intact so that you can retain access to this data even after your
          pipeline has been torn down.
        </p>
        <br />
        <figure>
          <img src="assets/images/demo/MISC_teardown_v2.gif">
          <figcaption>Running <em>tapestry teardown</em> from the command line.</figcaption>
        </figure>
        <br />
        <!--Section 6-->
        <h2>6 Implementation Challenges</h2>
        <br />
        <h3>6.1 Ingestion Phase Challenges</h3>
        <br />
        <h4>Scaling the EC2 Instance</h4>
        <br />
        <p>
          While prototyping the ingestion part of the pipeline, we encountered an interesting challenge. Initially, all
          of the Zoom data was being extracted and loaded into the data warehouse without any issues, but the Salesforce
          API call made by Airbyte was consistently timing out. This eventually led to a 504 server error.
        </p>
        <figure>
          <img src="assets/images/challenges/badgateway.png">
          <figcaption>504 bad gateway error on Airbyte.</figcaption>
        </figure>
        <br />
        <p>
          In AWS, we saw that the Airbyte EC2 Instance was becoming an unhealthy target for the Application Load
          Balancer every time we attempted to extract data from Salesforce through Airbyte.
        </p>
        <figure>
          <img src="assets/images/challenges/45_unhealthy_target.png" class="case-study-image">
          <figcaption>Unhealthy target for ALB on AWS console.</figcaption>
        </figure>
        <br />
        <p>
          After further investigation, we found that the EC2 instance had alarmingly high CPU usage. Our solution was to
          vertically scale this EC2 instance to increase its computing power. As we discussed
          in our architecture section, we were limited to deploying Airbye on a single EC2 instance, making it
          virtually impossible to horizontally scale. Once we increased the size of the server, our error message
          disappeared.
        <figure>
          <img src="assets/images/challenges/45_aws_cpu_spike.png" class="case-study-image">
          <figcaption>Spike in CPU utlization on AWS monitoring dashboard.</figcaption>
        </figure>
        <br />
        <p>
          This led to our decision to include a "CPU Utilization" section in Tapestry’s dashboard to monitor
          the AWS resources.
        </p>
        <br />
        <h4>Caching an API Response</h4>
        <p>
          While we were implementing the automation of connecting Salesforce to the data warehouse, we were getting inconsistent
          responses from API calls. When deploying each user's pipeline using the <code class="command">kickstart</code> command,
          Tapestry attempts to set up a connection between Salesforce and the user's warehouse through Airbyte. Ideally, the process is as follows:
        </p>    
          <figure>
            <img src="assets/images/challenges/new_salesforceAPI_1.png" class="case-study-image">
            <figcaption>Ideal setup via Airbyte API and data transfer.</figcaption>
          </figure>
          <br /> 
          <ol>
            <li>Tapestry makes an API call to Airbyte to retrieve the appropriate schema for Salesforce contacts.</li>
            <li>Airbyte makes an API call to Salesforce to get the contacts schema and send that schema back to Tapestry.</li>
            <li>Tapestry makes another API call to Airbyte to set up a connection between Salesforce and the data warehouse (Snowflake).</li>
          </ol>
          
          <p>
          The above process occurs once during deployment. If completed successfully, Airbyte makes API calls to Salesforce on the user's
          behalf to ingest data on a schedule.
           
          However, Airbyte did not always receive a successful response from Salesforce when requesting the schema.
          This, in turn, broke the rest of the process, and Tapestry was not able to establish the connection needed to begin
          transferring data.
        </p>
          <figure>
            <img src="assets/images/challenges/new_salesforceAPI_2.png" class="case-study-image">
            <figcaption>Error requesting Salesforce schema and subsequently cannot make API calls that follow.</figcaption>
          </figure>
          <br /> 
          <p>
            Upon further research, we found that Salesforce had an unreliable API endpoint, and that this was a common problem encountered by
            developers using Salesforce.
          </p>
          <br />
          <p>We couldn’t accept such an inconsistent response rate, so we considered two options:</p>
          <ul>
            <li>Implementing API retry logic</li>
            <li>Caching</li>
          </ul>
          
          <p>
            Airbyte already has built-in retry logic that attempts the API call three times. In addition, we made multiple
            requests manually.
            This resulted in us hitting Salesforce’s rate limit and being throttled for a 24-hour period.
          </p>
          <br />
          <p> 
            At that point, we could have tried exponentially backing off, but doing so would have made the user
            experience unacceptable.
            It would be an inconsistent experience, and would also potentially take hours before being able to ingest data.
          </p>
          <br />
          <p>
            So we turned to our next option, caching. We decided to store the Salesforce contact schema that we received from the next
            successful response in a file exporting a "contactSchema" object.
           </p>
            <figure>
              <img src="assets/images/challenges/get_source_schema.png" class="case-study-image">
              <figcaption>Logic using cached schema.</figcaption>
            </figure>
            <br />
            
          <p>
            As the above image shows, we can now reference the "contactSchema" object for future API calls. This improved the process dramatically.
          </p>
          
          <figure>
            <img src="assets/images/challenges/new_salesforceAPI_3.png" class="case-study-image">
            <figcaption>Tapestry caching solution for obtaining Salesforce schema.</figcaption>
          </figure>
          <br /> 
          <p>
            With the schema cached, we now only make one API call to establish the connection. 
          </p>
          <figure>
            <img src="assets/images/challenges/new_salesforceAPI_4.png" class="case-study-image">
            <figcaption>New request/response cycle during deployment with caching.</figcaption>
          </figure>
          <br /> 
           
          
        </p>
        
        <p>
          While this simplifies the overall deployment process, it's worth noting that this is not a permanent solution and comes with some drawbacks.
          If Salesforce ever makes a change to their schema, then our caching solution will break down. We would then need to capture the new
          schema and replace the
          old file with the new one. To us, though, this solution was worth the tradeoff of additional maintenance, and
          so far Salesforce has
          not made any changes to their schema.
        </p>
        <br />

        <h3>6.2 Storage Phase Challenge</h3>
        <br />
        <h4>Automating Warehouse Setup</h4>
        <br />
        <p>
          One interesting challenge we encountered when working with Snowflake was how to run SQL statements for a new
          user once they input their credentials. We needed to run a sequence of <em>34 commands</em> to set up the
          warehouse with the necessary databases, schema, roles, and permissions for the Tapestry pipeline to operate.
        </p>
        <br />
        <figure>
          <img src="assets/images/challenges/Example_SQL_Commands.png">
          <p></p>
          <figcaption>Example SQL statements required for warehouse setup.</figcaption>
        </figure>
        <br />
        <h4>Node.js SDK for Snowflake</h4>
        <p>
          We found that Snowflake provides a Node.js SDK, which would allow us to communicate with the Snowflake
          warehouse. Since the SDK only allows one SQL statement to be executed at a time, we needed to store each
          statement as an array element, and then iterate through each one individually.
        </p>
        <p>
          Now that we could execute each SQL statement individually, we ran into another problem because the SDK is
          callback-based and runs code asynchronously.
        </p>
        <br />
        <p>
          To visualize this problem, we added a log statement with the array index, and you can see how the statements
          are being executed out of order. The error logs indicated that Snowflake was attempting to create databases
          and
          tables before the permission and role statements had finished executing.
        </p>
        <figure>
          <img src="assets/images/challenges/snowflakeSDK_logsv3.png" class="case-study-image">
          <figcaption>SQL statements executed out of order.</figcaption>
        </figure>
        <br />
        <p>
          One option for solving this might be to nest each callback so that the statements are executed in the right
          order. However, this would result in unreadable code that would be hard to manage.
        </p>
        <br />
        <h4>Snowflake Promise Library</h4>
        <p>
          Instead, we opted to use the snowflake-promise library. This is a wrapper for the Snowflake SDK that provides
          a promised-based API.
        </p>
        <br />
        <p>
          This made it possible to use async/await to handle multiple promises in a synchronous fashion, and the logs
          indicate that the statements are being executed in the right order.
        </p>
        <figure>
          <img src="assets/images/challenges/snowflakepromise_logsv3.png" class="case-study-image">
          <figcaption>SQL statements executed in the correct order.</figcaption>
        </figure>
        <br />
        <h3>6.3 Syncing Phase Challenge</h3>
        <br />
        <h4>Injecting Secrets at Runtime</h4>
        <p>
          One particularly interesting challenge when deploying Grouparoo was determining how to inject sensitive user
          inputs, like API keys or passwords, into the Docker container for Grouparoo’s web application. For Grouparoo,
          we needed to reference these inputs in configuration files as environmental variables.
        </p>
        <br />
        <figure>
          <img src="assets/images/challenges/secrets_1.png">
          <figcaption>Challenge of injecting sensitive user inputs into the Docker container.</figcaption>
        </figure>
        <br />
        <p>
          We soon learned that you can pass environmental variables to a container by referencing a local .env file
          within a
          Docker Compose YAML file. This solved part of the problem. The Docker container could now access any variables
          we provided in this file. However, since we do not receive user inputs until runtime, this .env file
          had to be dynamically generated during execution, but before running the container on ECS.
        </p>
        <figure>
          <img src="assets/images/challenges/secrets_2.png">
          <figcaption>Using a .env file referenced by the Docker Compose YAML file to inject secrets at runtime.
          </figcaption>
        </figure>
        <br />
        <p>
          Our solution for doing this was two-fold. First, we stored the user inputs in the SSM Parameter Store on AWS.
          This ensured that this sensitive information was secure and encrypted, but also available for us to access
          from the AWS CLI as needed. Then, we created a function to dynamically write the .env file in the user’s
          project folder after we had all of their inputs.
        </p>
        <figure>
          <img src="assets/images/challenges/secrets_3.png">
          <figcaption>Storing secrets at runtime and dynamically generating a local .env file.</figcaption>
        </figure>
        <br />
        <!--Section 7-->
        <h2>7 Future Work</h2>
        <br />
        <p>
          There are still a few features we would like to add to Tapestry in the future.
        </p>
        <ul>
          <li>Enable cross-platform support, such as deployment on Google Cloud Platform.
          </li>
          <li>Deploy Airbyte with ECS, so that our user has greater flexibility in terms of scaling.
            Airbyte has indicated that this compatibility will be available sometime in the future.</li>
          <li>Create more built-in templates to the kickstart command, so that Tapestry can
            be used out-of-the-box for more use cases.</li>
          <li>Incorporate more advanced metrics for pipeline monitoring, such as Cloudwatch alarms that
            can send notifications when particular utilization thresholds are reached.</li>
        </ul>
        </br>
        <!-- Section 8 -->
        <h2>8 References</h2>

        <p><strong>Modern Data Pipeline:</strong></p>
        <ul>
          <li><a href="https://hevodata.com/blog/cloud-data-warehouse-101/" class="references">History of the Cloud Data
              Warehouse</a></li>
          <li><a href="https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/"
              class="references">Emerging Architectures for Modern Data Infrastructure</a></li>
          <li><a href="https://rudderstack.com/blog/the-complete-customer-data-stack" class="references">The Complete
              Customer Data Stack</a></li>
          <li><a href="https://dataled.academy/guides/modern-data-stack-for-growth/" class="references">Modern Data
              Stack for Growth</a></li>
        </ul>
        <p><strong>Data Silos:</strong></p>
        <ul>
          <li><a
              href="https://rudderstack.com/blog/heres-why-the-cloud-tools-used-by-marketing-sales-and-product-create-data-silos"
              class="references">Why Cloud Tools Create Data Silos</a></li>
        </ul>
        <p><strong>Reverse ETL/Syncing:</strong></p>
        <ul>
          <li><a href="https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb" class="references">Reverse ETL,
              A Primer</a></li>
          <li><a href="https://www.grouparoo.com/solutions/reverse-etl" class="references">Reverse ETL</a></li>
          <li><a href="https://hightouch.io/customers/zeplin/" class="references">Reverse ETL Case Study</a></li>
        </ul>
        <p><strong>Companies:</strong></p>
        <ul>
          <li><a href="https://airbyte.io/" class="references">Airbyte</a></li>
          <li><a href="https://www.grouparoo.com/" class="references">Grouparoo</a></li>
          <li><a href="https://www.snowflake.com/" class="references">Snowflake</a></li>
          <li> <a href="https://www.getdbt.com/" class="references">DBT</a></li>
          <li><a href="https://rudderstack.com/" class="references">Rudderstack</a></li>
        </ul>
        <br>
        <h2>Presentation</h2>
        <div class="presentation-wrapper case-study-image">

          <iframe class="presentation-video" frameborder="0" src="https://www.youtube.com/embed/nQdf8djNlFA"
            allowfullscreen></iframe>
        </div class="presentation-wrapper">
        <br>
        <h2>Team</h2>
        <br />
        <div class="section team-section">
          <div class="container">
            <div data-duration-in="300" data-duration-out="100" class="tabs w-tabs">
              <div data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a"
                style="transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1) rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg); transform-style: preserve-3d; opacity: 0;"
                class="tabs-content w-tab-content">
                <div>
                  <div class="team-grid">
                    <div class="team-member-wrap">
                      <img src="assets/images/team/katherine.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Katherine Beck</div>
                        <div class="team-member-location">Los Angeles, CA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:hello@katherinebeck.me" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="http://katherinebeck.me" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/katherine-m-beck/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/leah.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Leah Garrison</div>
                        <div class="team-member-location">Atlanta, GA</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:lgarrison1023@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://leahgarrison.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/leahgarrison/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/rickv2.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Rick Molé</div>
                        <div class="team-member-location">New York, NY</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:hello@rickmole.dev" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://rickmole.dev" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/rick-mole/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                    <div class="team-member-wrap">
                      <img src="assets/images/team/adam.png" loading="lazy" alt="">
                      <div class="team-member-info">
                        <div class="team-member-name">Adam Peterson</div>
                        <div class="team-member-location">Lexington, KY</div>
                      </div>
                      <ul class="team-member-icons">
                        <li>
                          <a href="mailto:adampeterson.tech@gmail.com" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-envelope"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://adampeterson.tech" target="_blank">
                            <span class="team-member-icon">
                              <i class="fas fa-globe"></i>
                            </span>
                          </a>
                        </li>
                        <li>
                          <a href="https://www.linkedin.com/in/adam-peterson-211a1041/" target="_blank">
                            <span class="team-member-icon">
                              <i class="fab fa-linkedin"></i>
                            </span>
                          </a>
                        </li>
                      </ul>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <br>
        <br>
    </article>
  </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
    type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
  <script src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
    type="text/javascript"></script>
  <script>
    /*!
     * toc - jQuery Table of Contents Plugin
     * v0.3.2
     * http://projects.jga.me/toc/
     * copyright Greg Allen 2014
     * MIT License
    */
    !function (a) { a.fn.smoothScroller = function (b) { b = a.extend({}, a.fn.smoothScroller.defaults, b); var c = a(this); return a(b.scrollEl).animate({ scrollTop: c.offset().top - a(b.scrollEl).offset().top - b.offset }, b.speed, b.ease, function () { var a = c.attr("id"); a.length && (history.pushState ? history.pushState(null, null, "#" + a) : document.location.hash = a), c.trigger("smoothScrollerComplete") }), this }, a.fn.smoothScroller.defaults = { speed: 400, ease: "swing", scrollEl: "body,html", offset: 0 }, a("body").on("click", "[data-smoothscroller]", function (b) { b.preventDefault(); var c = a(this).attr("href"); 0 === c.indexOf("#") && a(c).smoothScroller() }) }(jQuery), function (a) { var b = {}; a.fn.toc = function (b) { var c, d = this, e = a.extend({}, jQuery.fn.toc.defaults, b), f = a(e.container), g = a(e.selectors, f), h = [], i = e.activeClass, j = function (b, c) { if (e.smoothScrolling && "function" == typeof e.smoothScrolling) { b.preventDefault(); var f = a(b.target).attr("href"); e.smoothScrolling(f, e, c) } a("li", d).removeClass(i), a(b.target).parent().addClass(i) }, k = function () { c && clearTimeout(c), c = setTimeout(function () { for (var b, c = a(window).scrollTop(), f = Number.MAX_VALUE, g = 0, j = 0, k = h.length; k > j; j++) { var l = Math.abs(h[j] - c); f > l && (g = j, f = l) } a("li", d).removeClass(i), b = a("li:eq(" + g + ")", d).addClass(i), e.onHighlight(b) }, 50) }; return e.highlightOnScroll && (a(window).bind("scroll", k), k()), this.each(function () { var b = a(this), c = a(e.listType); g.each(function (d, f) { var g = a(f); h.push(g.offset().top - e.highlightOffset); var i = e.anchorName(d, f, e.prefix); if (f.id !== i) { a("<span/>").attr("id", i).insertBefore(g) } var l = a("<a/>").text(e.headerText(d, f, g)).attr("href", "#" + i).bind("click", function (c) { a(window).unbind("scroll", k), j(c, function () { a(window).bind("scroll", k) }), b.trigger("selected", a(this).attr("href")) }), m = a("<li/>").addClass(e.itemClass(d, f, g, e.prefix)).append(l); c.append(m) }), b.html(c) }) }, jQuery.fn.toc.defaults = { container: "body", listType: "<ul/>", selectors: "h1,h2,h3", smoothScrolling: function (b, c, d) { a(b).smoothScroller({ offset: c.scrollToOffset }).on("smoothScrollerComplete", function () { d() }) }, scrollToOffset: 0, prefix: "toc", activeClass: "toc-active", onHighlight: function () { }, highlightOnScroll: !0, highlightOffset: 100, anchorName: function (c, d, e) { if (d.id.length) return d.id; var f = a(d).text().replace(/[^a-z0-9]/gi, " ").replace(/\s+/g, "-").toLowerCase(); if (b[f]) { for (var g = 2; b[f + g];)g++; f = f + "-" + g } return b[f] = !0, e + "-" + f }, headerText: function (a, b, c) { return c.text() }, itemClass: function (a, b, c, d) { return d + "-" + c[0].tagName.toLowerCase() } } }(jQuery);
  </script>
  <script>
    /* initialize */
    $('.toc').toc({
      'selectors': 'h2', //elements to use as headings
      'container': 'article', //element to find all selectors in
      'smoothScrolling': true, //enable or disable smooth scrolling on click
      'prefix': 'toc', //prefix for anchor tags and class names
      'highlightOnScroll': true, //add class to heading that is currently in focus
      'highlightOffset': 100, //offset to trigger the next headline
    });
  </script>
</body>

</html>